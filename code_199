import numpy as np
from scipy.optimize._numdiff import approx_derivative

# Original cost function
def full_cost_new(vol, adv, w1, w2, K1, K2, x, nu):
    n = x.shape[0]
    imb1 = np.empty(n, dtype=np.float64)
    imb2 = np.empty(n, dtype=np.float64)
    curr1 = curr2 = 0.0
    for i in range(n):
        curr1 = curr1 * w1 + nu[i] * x[i]
        curr2 = curr2 * w2 + nu[i] * x[i]
        imb1[i] = curr1
        imb2[i] = curr2

    tmp = 0.0
    for i in range(n):
        imbalance = K1 * imb1[i] + K2 * imb2[i]
        tmp += np.sign(imbalance) * np.sqrt(abs(imbalance)) * x[i]

    return adv * np.sqrt(vol) * tmp

# Analytic gradient wrt x
def full_cost_new_jac_x(vol, adv, w1, w2, K1, K2, x, nu):
    n = x.shape[0]
    # build the prefix imbalances
    imb1 = np.empty(n)
    imb2 = np.empty(n)
    curr1 = curr2 = 0.0
    for i in range(n):
        curr1 = curr1 * w1 + nu[i] * x[i]
        curr2 = curr2 * w2 + nu[i] * x[i]
        imb1[i] = curr1
        imb2[i] = curr2
    imb = K1 * imb1 + K2 * imb2

    # f(imb) and its derivative
    f_imb = np.sign(imb) * np.sqrt(np.abs(imb))
    fprime = np.zeros_like(imb)
    nonzero = imb != 0
    fprime[nonzero] = 1.0 / (2.0 * np.sqrt(np.abs(imb[nonzero])))

    # assemble gradient
    g = np.zeros(n)
    for k in range(n):
        # direct partial from x_k * f(imb_k)
        direct = f_imb[k]

        # indirect part: sum over all i >= k
        sum_term = 0.0
        for i in range(k, n):
            coeff = K1 * (w1 ** (i - k)) + K2 * (w2 ** (i - k))
            sum_term += coeff * x[i] * fprime[i]
        indirect = nu[k] * sum_term

        g[k] = direct + indirect

    return adv * np.sqrt(vol) * g

# Test comparison
np.random.seed(0)
n = 7
vol, adv, w1, w2, K1, K2 = 2.5, 1.2, 0.8, 0.6, 1.1, 0.9
x = np.random.rand(n)
nu = np.random.rand(n)

# analytic
ana_grad = full_cost_new_jac_x(vol, adv, w1, w2, K1, K2, x, nu)

# numerical approximation
def F_of_x(xp):
    return full_cost_new(vol, adv, w1, w2, K1, K2, xp, nu)

num_grad = approx_derivative(F_of_x, x, method='2-point', rel_step=1e-6, abs_step=1e-6)

# display results
print("Analytic gradient:\n", ana_grad)
print("\nNumeric gradient:\n", num_grad)
print("\nMax absolute difference:", np.max(np.abs(ana_grad - num_grad)))
# fqi_nn_augmented.py – PyTorch GPU version (17 features, sampled c, ρ, tλ)
"""python
Monte‑Carlo Fitted‑Value Iteration with a neural network **(PyTorch, GPU)**
===========================================================================

Differences vs. the fixed‑param script:

* **Augmented parameters** sampled per state
  * `ρ  ~ U(0.8, 0.99)`
  * `c  ~ U(0, 10)`
  * `t_λ ~ U(1, 1000)`
* **17‑D feature vector** (constant, polynomials, sign kinks, cost interactions,
  raw params, spare slot).
* Streaming over the 41‑point action grid → low GPU memory (works on 8 GB).
* `V(0,0)` monitoring uses mid‑range params `(c = 5, ρ = 0.9, tλ = 500)`.

Copy & run as a script, or import and call `eval_policy()` for roll‑outs.
"""

# -----------------------------------------------------------------------------
# 0. Imports & device ----------------------------------------------------------
# -----------------------------------------------------------------------------

import math, time
import numpy as np
import torch, torch.nn as nn, torch.nn.functional as F

torch.manual_seed(0)
np.random.seed(0)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", DEVICE)

# -----------------------------------------------------------------------------
# 1. Hyper‑parameters & constants ---------------------------------------------
# -----------------------------------------------------------------------------

# Parameter ranges
aRHO_MIN, aRHO_MAX = 0.8, 0.99
cMIN, cMAX         = 0.0, 10.0
tlMIN, tlMAX       = 1.0, 1000.0
SIGMA_EPS          = 0.2
GAMMA              = 0.99

# Action grid
ACTIONS = torch.linspace(-1.0, 1.0, steps=41, device=DEVICE)  # (A,)
A = ACTIONS.numel()

# Training settings
N_DATASET    = 120_000
BATCH_SIZE   = 4_096
M_SAMPLES    = 100
N_ITERATIONS = 600
DATA_REFRESH = 50
LR           = 1e-3
WEIGHT_DECAY = 1e-5

# -----------------------------------------------------------------------------
# 2. Dataset sampler -----------------------------------------------------------
# -----------------------------------------------------------------------------

def resample_dataset(n=N_DATASET):
    p   = torch.randn(n, device=DEVICE)
    alpha = torch.randn(n, device=DEVICE)
    c   = (torch.rand(n, device=DEVICE) * (cMAX-cMIN) + cMIN)
    tl  = (torch.rand(n, device=DEVICE) * (tlMAX-tlMIN) + tlMIN)
    rho = (torch.rand(n, device=DEVICE) * (aRHO_MAX-aRHO_MIN) + aRHO_MIN)
    return p, alpha, c, tl, rho

p_data, a_data, c_data, tl_data, rho_data = resample_dataset()

# -----------------------------------------------------------------------------
# 3. 17‑D feature builder ------------------------------------------------------
# -----------------------------------------------------------------------------

_F = 17

@torch.jit.script
def features(p: torch.Tensor, a: torch.Tensor, c: torch.Tensor, rho: torch.Tensor, tl: torch.Tensor):
    sg_p  = torch.sign(p)
    sg_a  = torch.sign(a)
    return torch.stack([
        torch.ones_like(p),        # 0 const
        p, a, p*a,                # 1‑3
        p**2, a**2,               # 4‑5
        sg_p, sg_a, a*sg_p, p*sg_a,        # 6‑9
        c*torch.abs(p),           #10
        tl*p**2,                 #11
        c*torch.abs(a),          #12
        c, rho, tl,              #13‑15
        torch.zeros_like(p),     #16 spare
    ], dim=-1)                   # (...,17)

# -----------------------------------------------------------------------------
# 4. Reward function -----------------------------------------------------------
# -----------------------------------------------------------------------------

@torch.jit.script
def reward(alpha: torch.Tensor, p: torch.Tensor, x: torch.Tensor, c: torch.Tensor, tl: torch.Tensor):
    p_new = p + x
    return alpha*p_new - c*torch.abs(x) - 0.5*tl*x**2

# -----------------------------------------------------------------------------
# 5. Value network -------------------------------------------------------------
# -----------------------------------------------------------------------------

class ValueMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(_F,256), nn.ReLU(),
            nn.Linear(256,256), nn.ReLU(),
            nn.Linear(256,1)
        )
    def forward(self, phi):
        return self.net(phi).squeeze(-1)

model = ValueMLP().to(DEVICE)
opt   = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)

# -----------------------------------------------------------------------------
# 6. Training loop with streaming actions -------------------------------------
# -----------------------------------------------------------------------------

start = time.time()
for it in range(1, N_ITERATIONS+1):
    # refresh dataset
    if it == 1 or it % DATA_REFRESH == 0:
        p_data, a_data, c_data, tl_data, rho_data = resample_dataset()

    idx = torch.randint(0, N_DATASET, (BATCH_SIZE,), device=DEVICE)
    p   = p_data[idx]; alpha = a_data[idx]
    c   = c_data[idx]; tl = tl_data[idx]; rho = rho_data[idx]

    # current value V(s)
    V_s = model(features(p, alpha, c, rho, tl))  # (B,)

    # Monte‑Carlo next‑alpha (B,M)
    alpha_next = (
        alpha.unsqueeze(1)*rho.unsqueeze(1)
        + torch.randn(BATCH_SIZE, M_SAMPLES, device=DEVICE)*torch.sqrt(1-rho.unsqueeze(1)**2)
    )

    Q_best = torch.full((BATCH_SIZE,), -1e9, device=DEVICE)

    for a_trd in ACTIONS:  # loop 41 actions, keep memory small
        p_next = p + a_trd                       # (B,)
        p_AM   = p_next.unsqueeze(1).expand(-1, M_SAMPLES)

        # features of next state (B,M,17)
        phi_next = features(p_AM, alpha_next, c.unsqueeze(1), rho.unsqueeze(1), tl.unsqueeze(1))
        V_next = model(phi_next.reshape(-1,_F)).reshape(BATCH_SIZE, M_SAMPLES)  # (B,M)
        V_avg  = V_next.mean(dim=1)                 # (B,)

        R = reward(alpha, p, a_trd, c, tl)         # (B,)
        Q = R + GAMMA * V_avg
        Q_best = torch.maximum(Q_best, Q)

    # regression target y = Q_best
    loss = F.mse_loss(V_s, Q_best.detach())
    opt.zero_grad(); loss.backward(); opt.step()

    if it % 20 == 0:
        with torch.no_grad():
            phi00 = features(
                torch.tensor(0., device=DEVICE), torch.tensor(0., device=DEVICE),
                torch.tensor(5., device=DEVICE), torch.tensor(0.9, device=DEVICE), torch.tensor(500., device=DEVICE)
            ).unsqueeze(0)
            v00 = float(model(phi00))
        print(f"Iter {it:4d}/{N_ITERATIONS}  loss={loss.item():.4f}  V(0,0)={v00:+.5f}  |  {time.time()-start:.1f}s")

print("Training done ✔  Total time:", time.time()-start)

# -----------------------------------------------------------------------------
# 7. Greedy policy evaluation (MC per step) -----------------------------------
# -----------------------------------------------------------------------------

def eval_policy(model, *, fixed_c=5.0, fixed_corr=0.9, fixed_tl=500.0, m_samples=100, num_steps=20_000):
    model.eval()
    alpha_val = 0.0; p_val = 0.0
    pos_hist, rew_hist, alpha_hist = [p_val], [], [alpha_val]
    with torch.no_grad():
        for _ in range(num_steps):
            alpha_tensor = torch.tensor(alpha_val, device=DEVICE)
            p_tensor     = torch.tensor(p_val, device=DEVICE)

            # Monte‑Carlo next‑alpha (M,)
            eps = torch.randn(m_samples, device=DEVICE)
            alpha_next = alpha_tensor*fixed_corr + eps*math.sqrt(1-fixed_corr**2)

            Q_best = -1e9; best_a = 0.0
            for a_trd in ACTIONS:
                p_next = p_tensor + a_trd
                p_AM   = p_next.expand(m_samples)
                phi_next = features(p_AM, alpha_next,
                                    torch.tensor(fixed_c, device=DEVICE),
                                    torch.tensor(fixed_corr, device=DEVICE),
                                    torch.tensor(fixed_tl, device=DEVICE))
                V_next = model(phi_next)
                V_avg  = V_next.mean()
                R      = reward(alpha_tensor, p_tensor, a_trd, torch.tensor(fixed_c, device=DEVICE), torch.tensor(fixed_tl, device=DEVICE))
                Q      = float(R + GAMMA * V_avg)
                if Q > Q_best:
                    Q_best = Q; best_a = float(a_trd)

            r_step = float(reward(alpha_tensor, p_tensor, torch.tensor(best_a, device=DEVICE), torch.tensor(fixed_c, device=DEVICE), torch.tensor(fixed_tl, device=DEVICE)))
            rew_hist.append(r_step)
            p_val += best_a; pos_hist.append(p_val)
            alpha_val = fixed_corr*alpha_val + math.sqrt(1-fixed_corr**2)*np.random.randn()
            alpha_hist.append(alpha_val)
    return np.array(pos_hist), np.array(rew_hist), np.array(alpha_hist)

# ----------------------------------------------------------------------------
if __name__ == "__main__":
    pos, rew, alph = eval_policy(model, num_steps=5_000)
    print("Cumulative PnL:", rew.sum())

# ─── Cell [5]: Simulation with continuous AR(1) sampling for α₁, α₂ ──────────────
import math, torch
import matplotlib.pyplot as plt

# Simulation length
N_sim = 100_000

# Nearest‐neighbor helper
def nn_idx(grid: torch.Tensor, x: float) -> int:
    return int((grid - x).abs().argmin().item())

# Initialize continuous state
p_cont, i_cont = 0.0, 0.0
a1_cont, a2_cont = 0.0, 0.0

# Discrete indices
p_idx   = nn_idx(p_space, p_cont)
i_idx   = nn_idx(imb_space, i_cont)
a1_idx  = nn_idx(a_space, a1_cont)
a2_idx  = nn_idx(a_space, a2_cont)

# Precompute AR(1) noise scales
σ1 = math.sqrt(1 - rho1**2)
σ2 = math.sqrt(1 - rho2**2)

# Storage for cumulative reward
cum_rewards = []
cum_reward = 0.0

for t in range(N_sim):
    # 1) Sample the continuous α₁,α₂ via AR(1)
    a1_cont = rho1 * a1_cont + σ1 * torch.randn(1).item()
    a2_cont = rho2 * a2_cont + σ2 * torch.randn(1).item()
    #    discretize them
    a1_idx = nn_idx(a_space, a1_cont)
    a2_idx = nn_idx(a_space, a2_cont)

    # 2) Pick the greedy x from your policy
    x_idx = int(pi_opt[p_idx, a1_idx, a2_idx, i_idx].item())
    x     = float(x_space[x_idx].item())

    # 3) Instantaneous reward
    r = ((a_space[a1_idx] + a_space[a2_idx]) * (p_cont + x)
         - 0.5 * tla * (phi * i_cont + (1 - phi) * x) * x
         + C * abs(x)
         - 0.5 * la * (p_cont + x)**2)
    cum_reward += float(r)
    cum_rewards.append(cum_reward)

    # 4) Evolve continuous price & inventory
    p_cont = p_cont + x
    i_cont = phi * i_cont + (1 - phi) * x

    # 5) Discretize back for next step
    p_idx = nn_idx(p_space, p_cont)
    i_idx = nn_idx(imb_space, i_cont)

# ─── Plot ───────────────────────────────────────────────────────────────────────
plt.figure()
plt.plot(cum_rewards)
plt.xlabel("Time step")
plt.ylabel("Cumulative reward")
plt.title(f"Cumulative reward over {N_sim:,} steps")
plt.tight_layout()
plt.show()
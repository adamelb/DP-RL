import numpy as np
import matplotlib.pyplot as plt

# 1) PARAMETERS
phi1, phi2 = 0.8, 0.5       # imbalance decay factors
lambda_   = 0.1             # imbalance penalty weight
T         = 390             # minutes until close
n_alpha   = 3               # number of alphas

# 2) ALPHA AR(1) DYNAMICS
Rho = np.array([[0.9, 0.1, 0.0],
                [0.0, 0.7, 0.2],
                [0.0, 0.0, 0.5]])
Sigma_alpha = 0.01 * np.eye(n_alpha)

# 3) STATE‐SPACE MATRICES
# state s = [ p, imb1, imb2, α₁, α₂, α₃ ]ᵀ
n = 1 + 2 + n_alpha
F = np.zeros((n,n))
F[0,0] = 1             # p_{t+1} = p_t + x_t
F[1,1] = phi1          # imb1_{t+1}
F[2,2] = phi2          # imb2_{t+1}
F[3:,3:] = Rho         # alpha dynamics

G = np.zeros((n,1))
G[0,0] = 1
G[1,0] = 1 - phi1
G[2,0] = 1 - phi2

# 4) INSTANTANEOUS COST MATRICES
#   ℓ(s,x) = ½(p+x)² - (α₁/30)(p+x) + ½λ(imb1+imb2)x
Q = np.zeros((n,n))
Q[0,0] = 1

N = np.zeros((n,1))
# the term ½λ(imb1_{t+1}+imb2_{t+1}) x contributes cross‐terms:
N[1,0] = 0.5 * lambda_ * phi1
N[2,0] = 0.5 * lambda_ * phi2

R = 1 + lambda_ * (2 - (phi1 + phi2))

# 5) TERMINAL COST: V_T(s) = - p_T·(imb1_T+imb2_T) = sᵀ Qf s
Qf = np.zeros((n,n))
Qf[0,1] = Qf[1,0] = -0.5
Qf[0,2] = Qf[2,0] = -0.5

# 6) BACKWARD RICCATI‐LIKE RECURSION (no linear‐term q_t here)
P = [None]*(T+1)
r_const = np.zeros(T+1)    # accumulates trace‐terms of noise
K = [None]*T

P[T] = Qf

for t in reversed(range(T)):
    # denom: scalar H_t = R + Gᵀ P[t+1] G
    H = float(R + (G.T @ P[t+1] @ G))
    # state‐feedback gain K_t
    K[t] = ((G.T @ P[t+1] @ F + N.T) / H)
    # Riccati update
    P[t] = (Q
            + F.T @ P[t+1] @ F
            - (F.T @ P[t+1] @ G + N) @ K[t])
    # noise‐trace (for the constant term of V_t)
    # only alphas have noise
    Sigma_full = np.zeros((n,n))
    Sigma_full[3:,3:] = Sigma_alpha
    r_const[t] = r_const[t+1] + 0.5 * np.trace(P[t+1] @ Sigma_full)

# 7) SIMULATION UNDER OPTIMAL POLICY AND VALUE EVALUATION
np.random.seed(42)
s = np.zeros((n,1))
positions = []
values    = []
xs        = []
rewards   = []

for t in range(T):
    # extract α₁ and position components
    p, i1, i2 = s[0,0], s[1,0], s[2,0]
    alpha1    = s[3,0]

    # linear‐term from reward: c_t = -α₁/30
    c_t = -alpha1 / 30.0

    # recompute denom H_t
    H = float(R + (G.T @ P[t+1] @ G))
    # affine term k_t = (c_t) / H  (no q-term in this setup)
    k_t = c_t / H

    # policy: x_t = -K_t s_t - k_t
    x = float(-K[t] @ s - k_t)

    # record
    xs.append(x)
    positions.append(p)
    # value V_t(s) = sᵀ P_t s + r_const[t]  (quadratic + constant)
    V = float(s.T @ P[t] @ s) + r_const[t]
    values.append(V)

    # compute instantaneous reward for diagnostics
    p_next    = p + x
    im1_next  = phi1*i1 + (1-phi1)*x
    im2_next  = phi2*i2 + (1-phi2)*x
    rew = alpha1/30*p_next \
          - 0.5*p_next**2 \
          - 0.5*lambda_*(im1_next+im2_next)*x
    rewards.append(rew)

    # state update (noise only on alphas)
    eps = np.zeros((n,1))
    eps[3:,0] = np.random.multivariate_normal(np.zeros(n_alpha), Sigma_alpha)
    s = F @ s + G * x + eps

# 8) DIAGNOSTICS
print("Final cumulative position:", np.sum(xs))
plt.figure(figsize=(12,4))
plt.subplot(1,3,1)
plt.plot(np.cumsum(xs)); plt.title("Cumulative Position"); plt.grid(True)
plt.subplot(1,3,2)
plt.plot(values);          plt.title("V_t(s_t)");      plt.grid(True)
plt.subplot(1,3,3)
plt.plot(np.cumsum(rewards)); plt.title("Cumulative Reward"); plt.grid(True)
plt.tight_layout()
plt.show()
import torch
import torch.nn as nn
import torch.nn.functional as F

class SpecializedQNetwork(nn.Module):
    def __init__(self,
                 state_dim: int,
                 hidden_dim: int,
                 c_idx: int,
                 f1_c0_fn,           # callable: s->[batch] renvoyant f1_c0(s)
                 normalize: bool=False):
        """
        - state_dim : dimension de s (ici 25)
        - hidden_dim: taille des couches cachées
        - c_idx     : position de la feature c dans s (ici -5)
        - f1_c0_fn  : fonction externe qui calcule f1_c0(s)
        - normalize : si True, on renvoie [g1, f2/f1_c0, f3/f1_c0, f4/f1_c0]
        """
        super().__init__()
        self.c_idx       = c_idx
        self.f1_c0_fn    = f1_c0_fn
        self.normalize   = normalize

        # MLP simple qui sort 4 valeurs brutes : raw_g1, raw_g2, raw_f3, raw_f4
        self.backbone = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 4)
        )

    def forward(self, s: torch.Tensor):
        """
        s: [batch, state_dim]
        renvoie un tensor [batch,4] = [f1, f2, f3, f4] ou leurs normalisés
        """
        raw = self.backbone(s)          # [B,4]
        g1_raw, g2_raw, f3_raw, f4_raw = raw.unbind(dim=-1)

        # 1) g1 <= 0  via -softplus
        g1 = -F.softplus(g1_raw)        # [B]

        # 2) g2 >= 0  via softplus
        g2 = F.softplus(g2_raw)         # [B]

        # 3) récupère c(s)
        c = s[:, self.c_idx]            # [B]

        # 4) calcul f1_c0(s) externe
        f1_c0 = self.f1_c0_fn(s)        # [B]

        # 5) formules
        f1 = f1_c0 * g1                  # < 0 si g1<0
        f2 = -(1.0 + g2) * c
        f3 = f3_raw
        f4 = f4_raw

        if self.normalize:
            # on divise tout par f1_c0
            inv = 1.0 / (f1_c0 + 1e-8)
            f1 = g1                       # déjà f1/f1_c0 = g1
            f2 = f2 * inv
            f3 = f3 * inv
            f4 = f4 * inv

        return torch.stack([f1, f2, f3, f4], dim=-1)
## Decomposing an AR(0.7) process into four AR(ρᵢ) components

We wish to start from a zero-mean, unit‐variance AR(1) process
\[
X_t \;=\; 0.7\,X_{t-1} \;+\; \varepsilon_t,
\qquad \varepsilon_t \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0,\,1),
\]
so that 
\[
\mathrm{Cov}(X_t,X_{t-h}) \;=\; (0.7)^{\,|h|}, 
\quad \mathrm{Var}(X_t)=1,\quad h=0,1,2,\dots
\]
and attempt to write
\[
X_t \;=\; \sum_{i=1}^4 \alpha_i\,Y_t^{(i)},
\]
where each \(\{Y^{(i)}\}\) is itself a zero‐mean, unit‐variance AR(1) process with
\[
Y_t^{(i)} = \rho_i\,Y_{t-1}^{(i)} \;+\; \varepsilon_t^{(i)}, 
\quad \mathrm{Var}\bigl(Y_t^{(i)}\bigr)=1,
\]
for the four fixed values
\[
\rho_1 \;=\; 0,\quad
\rho_2 \;=\;0.44,\quad
\rho_3 \;=\;0.85,\quad
\rho_4 \;=\;0.95.
\]
We want to know: **Can we choose real scalars \(\{\alpha_i\}\) so that the *sum* \(X_t=\sum_i \alpha_i\,Y_t^{(i)}\) itself satisfies exactly \(X_t=0.7\,X_{t-1}+\varepsilon_t\) (unit‐variance)?**  If not, can we at least match the autocovariance function up to some finite lag?

---

### 1. Writing \(X_t\) as a sum of four AR(ρᵢ) “basis” processes

Suppose we set
\[
X_t \;=\; \sum_{i=1}^4 \alpha_i\,Y_t^{(i)}, 
\quad\text{where each}\quad
Y_t^{(i)} = \rho_i\,Y_{t-1}^{(i)} \;+\; \varepsilon_t^{(i)}, 
\quad \mathrm{Var}\bigl(Y_t^{(i)}\bigr)=1,
\]
and \(\{\varepsilon_t^{(i)}\}_{t,i}\) are mutually independent white noises, each \(\mathcal{N}(0,1)\).  Then:

1.  Each \(Y^{(i)}\) has
   \[
   \mathrm{Cov}\bigl(Y_t^{(i)},\,Y_{t-h}^{(i)}\bigr)
   \;=\; \rho_i^{\,|h|}, \quad
   \mathrm{Var}\bigl(Y_t^{(i)}\bigr)=1.
   \]
   In particular, \(\mathrm{Cov}(Y_t^{(i)},Y_{t-1}^{(i)})=\rho_i.\)

2.  Because the four \(Y^{(i)}\) are independent, the variance of \(X_t\) is
   \[
   \mathrm{Var}(X_t)
   \;=\; \sum_{i=1}^4 \alpha_i^2\,\mathrm{Var}\bigl(Y_t^{(i)}\bigr)
   \;=\; \sum_{i=1}^4 \alpha_i^2.
   \]
   We will eventually insist that \(\mathrm{Var}(X_t)=1.\)

3.  The one‐step‐lag covariance of \(X\) is
   \[
   \mathrm{Cov}(X_t,\,X_{t-1})
   \;=\;\mathrm{Cov}\Bigl(\sum_{i=1}^4 \alpha_i\,Y_t^{(i)}, \,\sum_{j=1}^4 \alpha_j\,Y_{t-1}^{(j)}\Bigr)
   \;=\;\sum_{i=1}^4 \alpha_i^2\,\rho_i.
   \]
   Hence the implied lag‐1 autocorrelation of \(X\) is
   \[
   \rho_X(1)
   \;=\;\frac{\mathrm{Cov}(X_t,\,X_{t-1})}{\mathrm{Var}(X_t)}
   \;=\;\frac{\sum_{i=1}^4 \alpha_i^2\,\rho_i}{\sum_{i=1}^4 \alpha_i^2}.
   \]

4.  More generally, for any \(h\ge0\),
   \[
   \mathrm{Cov}(X_t,\,X_{t-h})
   \;=\;\sum_{i=1}^4 \alpha_i^2\,\rho_i^{\,|h|}, 
   \]
   so
   \[
   \rho_X(h)
   \;=\;\frac{\sum_{i=1}^4 \alpha_i^2\,\rho_i^{\,|h|}}{\sum_{i=1}^4 \alpha_i^2}.
   \]
   If we also impose \(\sum_{i=1}^4 \alpha_i^2=1\), then simply
   \[
   \rho_X(h) \;=\;\sum_{i=1}^4 (\alpha_i^2)\,\rho_i^{\,|h|}.
   \]

---

### 2. Can one force \(X_t\) to satisfy the *AR(0.7)* recursion exactly?

In order for \(X_t\) itself to satisfy
\[
X_t \;=\; 0.7\,X_{t-1} \;+\; \varepsilon_t,\qquad
\mathrm{Var}(\varepsilon_t)=1,
\]
it is necessary and sufficient that
\[
X_t - 0.7\,X_{t-1} \;=\; \varepsilon_t
\;\Longleftrightarrow\;
\sum_{i=1}^4 \alpha_i \Bigl(Y_t^{(i)} - 0.7\,Y_{t-1}^{(i)}\Bigr)
\;=\;\varepsilon_t.
\]
But each \(Y_t^{(i)}\) itself satisfies
\[
Y_t^{(i)} - \rho_i\,Y_{t-1}^{(i)} \;=\; \varepsilon_t^{(i)}.
\]
Therefore,
\[
Y_t^{(i)} - 0.7\,Y_{t-1}^{(i)}
\;=\;
(\rho_i - 0.7)\,Y_{t-1}^{(i)}
\;+\;\varepsilon_t^{(i)}.
\]
Hence
\[
X_t - 0.7\,X_{t-1}
=\sum_{i=1}^4 \alpha_i\,\bigl[(\rho_i - 0.7)\,Y_{t-1}^{(i)} + \varepsilon_t^{(i)}\bigr]
=\sum_{i=1}^4 \alpha_i(\rho_i - 0.7)\,Y_{t-1}^{(i)} \;+\; \sum_{i=1}^4 \alpha_i\,\varepsilon_t^{(i)}.
\]
For this to equal a \emph{single} white‐noise \(\varepsilon_t\) (with \(\mathrm{Var}=1\)) for \emph{all} values of the past \(\{Y_{t-1}^{(i)}\}\), we would require
\[
\sum_{i=1}^4 \alpha_i\,(\rho_i - 0.7)\,Y_{t-1}^{(i)} \equiv 0
\quad\text{for all possible }(Y_{t-1}^{(1)},\dots,Y_{t-1}^{(4)}).
\]
Since \(\{Y_{t-1}^{(i)}\}\) can be (jointly) anything in \(\mathbb{R}^4\), the only way
\(\alpha_i(\rho_i - 0.7)\,Y_{t-1}^{(i)}\) can vanish identically is if, for each \(i\),
\[
\alpha_i\,(\rho_i - 0.7) = 0.
\]
But our fixed \(\{\rho_i\}\) are
\[
\rho_1 = 0,\;\rho_2 = 0.44,\;\rho_3 = 0.85,\;\rho_4 = 0.95,
\]
none of which equals \(0.7\).  Therefore, for each \(i\) we have \(\rho_i - 0.7 \neq 0\).  Consequently, we would be forced to set
\[
\alpha_i = 0\quad\text{for all }i=1,2,3,4,
\]
in order to kill the term \(\alpha_i(\rho_i - 0.7)\,Y_{t-1}^{(i)}\).  But that makes \(X_t\equiv 0\), which is not the AR(0.7) we started with.  Hence:

> **There is no choice of real constants \(\{\alpha_i\}\) that makes \(\sum_{i=1}^4 \alpha_i\,Y_t^{(i)}\) satisfy \(X_t=0.7\,X_{t-1}+\varepsilon_t\) exactly.**  

In other words, *one cannot decompose an AR(0.7) process exactly as a finite sum of four AR(ρᵢ) processes with \(\rho_i \neq 0.7\).*  

---

### 3. Matching the autocovariance function up to a finite lag

Even though we cannot force the exact AR‐recursion, we can ask: **Can we choose \(\{\alpha_i\}\) so that the *autocovariance* of \(X_t=\sum_i\alpha_i Y_t^{(i)}\) matches that of AR(0.7) for, say, \(h=0,1,2,3\)?**  If so, one might still call that a “decomposition with approximately the same ACF up to lag 3.”  We now show how to do this “ACF‐matching” for lags 0–3 and see why the real solution forces some \(\alpha_i^2<0\).

1.  **Autocovariances of \(X_t\).**  Since the \(Y^{(i)}\) are independent and each has \(\mathrm{Var}(Y_t^{(i)})=1\), 
    \[
    \gamma_X(h) \;=\; \mathrm{Cov}(X_t,\,X_{t-h})
    \;=\; \sum_{i=1}^4 \alpha_i^2\,\rho_i^{\,|h|}, 
    \quad h=0,1,2,\dots
    \]
    In particular,
    \[
    \gamma_X(0) \;=\; \sum_{i=1}^4 \alpha_i^2,\qquad
    \gamma_X(1) \;=\; \sum_{i=1}^4 \alpha_i^2\,\rho_i,\qquad
    \gamma_X(2) \;=\;\sum_{i=1}^4 \alpha_i^2\,\rho_i^2,\quad\text{etc.}
    \]
    We also want \(\mathrm{Var}(X_t)=1\), so we will insist
    \[
    \sum_{i=1}^4 \alpha_i^2 \;=\; 1.
    \]
    Hence the \emph{autocorrelation} of \(X\) at lag \(h\) is
    \[
    \rho_X(h) \;=\;\frac{\gamma_X(h)}{\gamma_X(0)} 
    \;=\; \sum_{i=1}^4 \bigl(\alpha_i^2\bigr)\,\rho_i^{\,|h|}.
    \]
    We wish to make \(\rho_X(h) = (0.7)^{\,h}\) for \(h=0,1,2,3\).  Since \(\rho_X(0)\equiv1\) always holds if \(\sum \alpha_i^2=1\), the nontrivial constraints are:

    - **Lag 1:**  \(\displaystyle \rho_X(1) \;=\;\sum_{i=1}^4 \alpha_i^2\,\rho_i \;=\; 0.7.\)
    - **Lag 2:**  \(\displaystyle \rho_X(2) \;=\;\sum_{i=1}^4 \alpha_i^2\,\rho_i^2 \;=\; 0.7^2=0.49.\)
    - **Lag 3:**  \(\displaystyle \rho_X(3) \;=\;\sum_{i=1}^4 \alpha_i^2\,\rho_i^3 \;=\; 0.7^3=0.343.\)

2.  **Rewriting in terms of “weights” \(w_i = \alpha_i^2.\)**  Since \(\alpha_i^2\ge0\) (for real \(\alpha_i\)), define
    \[
    w_i \;=\;\alpha_i^2,\quad i=1,2,3,4.
    \]
    Then the constraints become
    \[
    \tag{1}
    \begin{cases}
    w_1 + w_2 + w_3 + w_4 \;=\; 1,\\[0.5em]
    w_1\,\rho_1 + w_2\,\rho_2 + w_3\,\rho_3 + w_4\,\rho_4 \;=\; 0.7,\\[0.5em]
    w_1\,\rho_1^2 + w_2\,\rho_2^2 + w_3\,\rho_3^2 + w_4\,\rho_4^2 \;=\; 0.49,\\[0.5em]
    w_1\,\rho_1^3 + w_2\,\rho_2^3 + w_3\,\rho_3^3 + w_4\,\rho_4^3 \;=\; 0.343,
    \end{cases}
    \]
    with
    \[
    (\rho_1,\rho_2,\rho_3,\rho_4) \;=\; (0,\;0.44,\;0.85,\;0.95).
    \]
    Observe that for \(\rho_1=0\), any term involving \(\rho_1^k\) with \(k\ge1\) vanishes.

3.  **Plugging in \(\rho_i\) and simplifying.**  Since \(\rho_1=0\), the equations (1) become
    \[
    \begin{cases}
    w_1 + w_2 + w_3 + w_4 = 1,\\[0.5em]
    0\cdot w_1 + 0.44\,w_2 + 0.85\,w_3 + 0.95\,w_4 = 0.7,\\[0.5em]
    0^2\cdot w_1 + (0.44)^2\,w_2 + (0.85)^2\,w_3 + (0.95)^2\,w_4 = 0.49,\\[0.5em]
    0^3\cdot w_1 + (0.44)^3\,w_2 + (0.85)^3\,w_3 + (0.95)^3\,w_4 = 0.343.
    \end{cases}
    \]
    In explicit numerical form:
    \[
    \begin{cases}
    w_1 + w_2 + w_3 + w_4 = 1,\\[0.5em]
    0.44\,w_2 + 0.85\,w_3 + 0.95\,w_4 = 0.7,\\[0.5em]
    0.1936\,w_2 + 0.7225\,w_3 + 0.9025\,w_4 = 0.49,\\[0.5em]
    0.085184\,w_2 + 0.614125\,w_3 + 0.857375\,w_4 = 0.343.
    \end{cases}
    \]
    We see that \(w_1\) appears only in the first equation, while \((w_2,w_3,w_4)\) appear in all four.  Solve by writing
    \[
    w_1 \;=\; 1 \;-\;(w_2 + w_3 + w_4),
    \]
    and then the remaining three equations form a \(3\times3\) linear system in \((w_2,w_3,w_4)\):
    \[
    \underbrace{\begin{pmatrix}
    0.44 & 0.85 & 0.95\\[0.3em]
    0.1936 & 0.7225 & 0.9025\\[0.3em]
    0.085184 & 0.614125 & 0.857375
    \end{pmatrix}}_{=:A}
    \begin{pmatrix}w_2\\w_3\\w_4\end{pmatrix}
    \;=\;
    \begin{pmatrix}0.7\\0.49\\0.343\end{pmatrix}.
    \]
    One checks (e.g.\ by inverting \(A\) numerically) that the unique solution is
    \[
    (w_2,w_3,w_4)
    \;\approx\;(0.2853137,\;1.3055954,\;-0.5634675).
    \]
    Then
    \[
    w_1 \;=\; 1 \;-\;(w_2 + w_3 + w_4)
    \;\approx\; 1 \;-\;(0.2853137 + 1.3055954 - 0.5634675)
    \;=\;-0.0274416.
    \]
    Hence
    \[
    (w_1,w_2,w_3,w_4)
    \;\approx\;
    \bigl(-0.0274416,\;0.2853137,\;1.3055954,\;-0.5634675\bigr).
    \]
    Two of these \(w_i\) are negative (\(w_1\) and \(w_4\)), which means there is **no choice of real** \(\alpha_i\) with \(\alpha_i^2=w_i\) for those indices.  In particular:

    > **No real, nonnegative solution** \(\{w_i\}_{i=1}^4\) exists that satisfies all four equations for lags \(0\le h\le3\).  The unique algebraic solution forces \(w_1<0\) and \(w_4<0\).  

4.  **Consequence.**  Since \(w_i=\alpha_i^2\) must be nonnegative for \(\alpha_i\in\mathbb{R}\), we conclude:

    - There is **no way** to match \(\rho_X(h)=(0.7)^h\) {\it exactly} for \(h=0,1,2,3\) using four real AR(ρᵢ) components.  
    - In fact, there is **no real decomposition** into four unit‐variance AR(ρᵢ) with \(\rho_i\in\{0,0.44,0.85,0.95\}\) that reproduces the AR(0.7) autocovariance out to lag 3.  

---

### 4. Interpretation and approximate matching

1.  To match \(\rho_X(1)=0.7\) **only**, we need merely two constraints:
    \[
    \begin{cases}
    w_1 + w_2 + w_3 + w_4 = 1,\\[0.5em]
    0.44\,w_2 + 0.85\,w_3 + 0.95\,w_4 = 0.7,
    \end{cases}
    \]
    with \(w_i\ge0\).  Any nonnegative \(\{w_i\}\) satisfying these two is a valid “lag‐1 match.”  For example, set
    \[
    w_1 = 0,\quad
    w_2 = 0,\quad
    w_3 = \frac{0.7 - 0.95\,w_4}{0.85},\quad
    w_4 = 0.2,
    \]
    and then adjust to make \(\sum w_i=1\).  Concretely, choose
    \[
    w_3 = 0.8235294,\quad
    w_4 = 0.1764706,\quad
    w_1=w_2=0,
    \]
    since \(0.8235294\cdot 0.85 + 0.1764706\cdot 0.95 = 0.7\) and \(0.8235294+0.1764706=1\).  Then \(\alpha_3=\sqrt{0.8235294}\), \(\alpha_4=\sqrt{0.1764706}\), \(\alpha_1=\alpha_2=0\) yields a real solution with \(\rho_X(1)=0.7\), albeit \(\rho_X(2)\neq0.49\) etc.

2.  To “approximate” AR(0.7) as closely as possible via four exponentials \(\{\rho_i^h\}\), one must decide which lags to match exactly and which to leave as an approximation.  Matching **four** lags requires solving a \(4\times4\) system in \(\{w_i\}\).  We found that system has no real, nonnegative solution given \(\{\rho_i\}=\{0,0
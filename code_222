import math
from dataclasses import dataclass
from typing import Optional, Tuple, Dict

import torch
import torch.nn as nn
import torch.nn.functional as F


# ---------------------------
# Config
# ---------------------------
@dataclass
class ValueConfig:
    # Market/impact params
    k1: float = 1.0
    k2: float = 1.0
    phi1: float = 0.95          # imbalance persistence for level 1
    phi2: float = 0.95          # imbalance persistence for level 2
    c_abs: float = 0.0          # cost coefficient for |x|
    gamma: float = 0.7          # your gamma
    w: float = 0.3              # your w

    # Feature & model
    use_residual_mlp: bool = True
    residual_hidden: int = 32
    residual_layers: int = 2
    dropout: float = 0.0

    # Training
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    lr: float = 3e-4
    weight_decay: float = 1e-6

    # Action grid for Bellman backups (continuous trade size grid)
    # e.g., torch.linspace(-1.0, 1.0, 201)
    action_min: float = -1.0
    action_max: float = 1.0
    action_steps: int = 201

    # Monte Carlo next-state rollouts (if you add noise)
    mc_next: int = 1            # set >1 if you inject stochasticity into p, alpha, etc.


# ---------------------------
# Utilities
# ---------------------------
def pos_part(x: torch.Tensor) -> torch.Tensor:
    return torch.clamp(x, min=0.0)

def neg_part(x: torch.Tensor) -> torch.Tensor:
    return torch.clamp(-x, min=0.0)

def safe_sqrt(x: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
    return torch.sqrt(torch.clamp(x, min=0.0) + eps)


# ---------------------------
# Feature engineering φ(s)
# ---------------------------
class FeatureMap(nn.Module):
    """
    Input state s = [p, alpha, imb1_plus, imb1_minus, imb2_plus, imb2_minus]
    Produces φ(s) with impact-shaped features.
    """
    def __init__(self, cfg: ValueConfig):
        super().__init__()
        self.cfg = cfg

    def forward(self, s: torch.Tensor) -> torch.Tensor:
        # s: [B, 6]
        p, a, i1p, i1m, i2p, i2m = torch.unbind(s, dim=-1)

        # Aggregate signed/unsigned imbalances
        m = self.cfg.k1 * (i1p - i1m) + self.cfg.k2 * (i2p - i2m)   # signed pressure
        u = self.cfg.k1 * (i1p + i1m) + self.cfg.k2 * (i2p + i2m)   # total intensity

        m_pos = pos_part(m)
        m_neg = pos_part(-m)

        # Core features
        feats = [
            torch.ones_like(p),     # 1
            p, a, m, u,
            p*p, a*a, m*m, u*u,
            p*a, p*m, p*u, a*m, a*u, m*u,
            safe_sqrt(m_pos), safe_sqrt(m_neg),
            safe_sqrt(u), torch.log1p(u),
            a * safe_sqrt(u),
        ]

        # Optional decay-aware terms (comment in if useful)
        # i1_signed = i1p - i1m
        # i2_signed = i2p - i2m
        # i1_uns = i1p + i1m
        # i2_uns = i2p + i2m
        # feats += [
        #     self.cfg.phi1 * i1_signed, self.cfg.phi2 * i2_signed,
        #     self.cfg.phi1 * i1_uns,    self.cfg.phi2 * i2_uns
        # ]

        return torch.stack(feats, dim=-1)  # [B, F]


# ---------------------------
# Value model: linear + residual
# ---------------------------
class ResidualMLP(nn.Module):
    def __init__(self, in_dim: int, hidden: int, layers: int, dropout: float = 0.0):
        super().__init__()
        L = []
        d = in_dim
        for _ in range(layers):
            L += [nn.Linear(d, hidden), nn.ReLU()]
            if dropout > 0:
                L += [nn.Dropout(dropout)]
            d = hidden
        L += [nn.Linear(d, 1)]
        self.net = nn.Sequential(*L)

        # Kaiming init
        for m in self.net:
            if isinstance(m, nn.Linear):
                nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))
                if m.bias is not None:
                    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)
                    bound = 1 / math.sqrt(fan_in)
                    nn.init.uniform_(m.bias, -bound, bound)

    def forward(self, x):
        return self.net(x).squeeze(-1)


class ValueNet(nn.Module):
    """
    V(s) = beta^T φ(s) + g_theta(φ(s))   (residual optional)
    """
    def __init__(self, cfg: ValueConfig, feature_dim: Optional[int] = None):
        super().__init__()
        self.cfg = cfg
        self.feats = FeatureMap(cfg)
        # Determine feature_dim by a dummy forward if not provided
        if feature_dim is None:
            with torch.no_grad():
                dummy = torch.zeros(1, 6)
                feature_dim = self.feats(dummy).shape[-1]
        self.feature_dim = feature_dim

        self.beta = nn.Linear(self.feature_dim, 1, bias=False)  # linear backbone
        nn.init.zeros_(self.beta.weight)

        self.use_residual = cfg.use_residual_mlp
        if self.use_residual:
            self.residual = ResidualMLP(
                in_dim=self.feature_dim,
                hidden=cfg.residual_hidden,
                layers=cfg.residual_layers,
                dropout=cfg.dropout
            )
        else:
            self.residual = None

    def V(self, s: torch.Tensor) -> torch.Tensor:
        phi = self.feats(s)
        v = self.beta(phi).squeeze(-1)
        if self.residual is not None:
            v = v + self.residual(phi)
        return v

    def forward(self, s: torch.Tensor) -> torch.Tensor:
        return self.V(s)


# ---------------------------
# Impact & reward
# ---------------------------
class TradingMechanics:
    """
    Encapsulates the reward and state transition for imbalances.
    You can extend to include p, alpha dynamics if needed.
    """
    def __init__(self, cfg: ValueConfig):
        self.cfg = cfg

    def transition_imbalances(self, s: torch.Tensor, x: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Apply your imbalance decay + the trade x (x_plus, x_minus).
        imb1+_new = phi1 * imb1+ + x_plus
        imb1-_new = phi1 * imb1- - x_minus
        imb2: same with phi2
        """
        p, a, i1p, i1m, i2p, i2m = torch.unbind(s, dim=-1)
        x_plus = pos_part(x)
        x_minus = pos_part(-x)

        i1p_new = self.cfg.phi1 * i1p + x_plus
        i1m_new = self.cfg.phi1 * i1m - x_minus
        i2p_new = self.cfg.phi2 * i2p + x_plus
        i2m_new = self.cfg.phi2 * i2m - x_minus

        s_new = torch.stack([p, a, i1p_new, i1m_new, i2p_new, i2m_new], dim=-1)
        aux = {"x_plus": x_plus, "x_minus": x_minus}
        return s_new, aux

    def impact_cost_components(self, s: torch.Tensor, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        # recompute aggregate + components needed for impact shapes
        _, _, i1p, i1m, i2p, i2m = torch.unbind(s, dim=-1)
        x_plus = pos_part(x)
        x_minus = pos_part(-x)

        imb_plus = self.cfg.k1 * i1p + self.cfg.k2 * i2p   # only buys side
        imb_minus = self.cfg.k1 * i1m + self.cfg.k2 * i2m  # only sells side

        # Per your example: Ipct_cost_buys = sqrt(imb_plus) * x_plus
        ipct_buys = safe_sqrt(imb_plus) * x_plus
        ipct_sells = safe_sqrt(imb_minus) * x_minus

        # Total-imbalances channel (if you also keep a combined form)
        # (If you want a variant using combined unsigned imbalances)
        total_imbal = self.cfg.k1 * (i1p + i1m) + self.cfg.k2 * (i2p + i2m)
        ipct_total = safe_sqrt(total_imbal) * (x_plus + x_minus)

        return {
            "ipct_buys": ipct_buys,
            "ipct_sells": ipct_sells,
            "ipct_total": ipct_total,
        }

    def impact_cost(self, s: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
        comps = self.impact_cost_components(s, x)
        # gamma * ( w * total_imbalances + (1-w)*(split buys/sells) )
        blended = self.cfg.w * comps["ipct_total"] + (1.0 - self.cfg.w) * (comps["ipct_buys"] + comps["ipct_sells"])
        return self.cfg.gamma * blended

    def reward(self, s: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
        """
        r(s,x) = alpha * (p + x) - c|x| - impact_cost(s, x)
        """
        p, a, *_ = torch.unbind(s, dim=-1)
        cost_abs = self.cfg.c_abs * (pos_part(x) + pos_part(-x))
        return a * (p + x) - cost_abs - self.impact_cost(s, x)


# ---------------------------
# Bellman backup utilities
# ---------------------------
class Bellman:
    def __init__(self, cfg: ValueConfig, mech: TradingMechanics, v_next: ValueNet):
        self.cfg = cfg
        self.mech = mech
        self.v_next = v_next  # frozen (no grad)

        # Action grid (deterministic MPC-like evaluation)
        self.action_grid = torch.linspace(cfg.action_min, cfg.action_max, cfg.action_steps, device=cfg.device)

    @torch.no_grad()
    def target_V(self, s_batch: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Compute V-target(s) = max_x [ r(s,x) + V_next(s') ] over a fixed action grid.
        Returns (targets, argmax_x).
        """
        B = s_batch.shape[0]
        X = self.action_grid.view(1, -1).expand(B, -1)  # [B, A]

        # Broadcast states for all actions
        s_exp = s_batch.unsqueeze(1).expand(B, X.shape[1], s_batch.shape[1])  # [B, A, 6]
        x_exp = X  # [B, A]

        # Transition imbalances deterministically (extend for stochastic p/alpha if needed)
        s_next, _ = self.mech.transition_imbalances(s_exp.reshape(-1, 6), x_exp.reshape(-1))
        # If you have dynamics for p and alpha, add them here before evaluating V_next.

        r = self.mech.reward(s_exp.reshape(-1, 6), x_exp.reshape(-1))  # [B*A]
        v_next = self.v_next.V(s_next)  # [B*A]

        q = r + v_next
        q = q.view(B, -1)  # [B, A]

        q_max, idx = torch.max(q, dim=1)  # [B], [B]
        x_star = X[torch.arange(B, device=X.device), idx]  # greedy action
        return q_max, x_star


# ---------------------------
# Trainer for one VI step
# ---------------------------
class ValueTrainer:
    def __init__(self, cfg: ValueConfig, value_net: ValueNet):
        self.cfg = cfg
        self.value_net = value_net.to(cfg.device)
        self.opt = torch.optim.AdamW(self.value_net.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)

    def fit(self, s_batch: torch.Tensor, targets: torch.Tensor, epochs: int = 2, batch_size: int = 8192):
        """
        Train to regression targets (MSE).
        """
        self.value_net.train()
        s = s_batch.to(self.cfg.device)
        y = targets.to(self.cfg.device)

        N = s.shape[0]
        for epoch in range(epochs):
            perm = torch.randperm(N, device=self.cfg.device)
            for i in range(0, N, batch_size):
                idx = perm[i:i+batch_size]
                s_i, y_i = s[idx], y[idx]
                pred = self.value_net.V(s_i)
                loss = F.mse_loss(pred, y_i)

                self.opt.zero_grad(set_to_none=True)
                loss.backward()
                self.opt.step()


# ---------------------------
# Example usage (sketch)
# ---------------------------
if __name__ == "__main__":
    cfg = ValueConfig()
    mech = TradingMechanics(cfg)

    # Initialize V_{t+1} (frozen) and V_t (trainable). For first iteration you can use zeros.
    V_next = ValueNet(cfg)
    for p in V_next.parameters():
        p.requires_grad_(False)
    V_t = ValueNet(cfg)

    # Sample a dataset of states s ~ D_t (covering wide state-space)
    # Here we just make something up; replace with your sampler.
    N = 100000
    torch.manual_seed(0)
    p = torch.randn(N) * 0.5 + 100.0
    a = torch.randn(N) * 0.01
    i1p = torch.rand(N) * 2.0
    i1m = torch.rand(N) * 2.0
    i2p = torch.rand(N) * 2.0
    i2m = torch.rand(N) * 2.0
    S = torch.stack([p, a, i1p, i1m, i2p, i2m], dim=-1).to(cfg.device)

    # Build Bellman targets using V_{t+1}
    bell = Bellman(cfg, mech, V_next)
    with torch.no_grad():
        targets, x_star = bell.target_V(S)

    # Fit V_t to the targets
    trainer = ValueTrainer(cfg, V_t)
    trainer.fit(S, targets, epochs=2, batch_size=8192)

    # V_t is now trained for this iterat
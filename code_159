import numpy as np
import matplotlib.pyplot as plt

# ---------------------------------------------------
# 0) EXAMPLE STD ARRAY (replace with your real stds)
#    stds.shape == (389,3), columns = [std(alpha1), std(alpha2), std(alpha3)]
# ---------------------------------------------------
np.random.seed(0)
stds = np.abs(np.random.randn(389, 3))  # demo; you will supply yours

# ---------------------------------------------------
# 1) SETUP
# ---------------------------------------------------
T       = stds.shape[0]      # 389
phi1, phi2 = 0.8, 0.5        # imbalance decay
n_alpha   = stds.shape[1]    # 3
n         = 1 + 2 + n_alpha  # state dim = [p, imb1, imb2, α1, α2, α3]

# AR(1) for alphas (you will replace with your Rho/Sigma)
Rho         = np.array([[0.9,0.05,0.0],
                        [0.0,0.8, 0.1],
                        [0.0,0.0, 0.7]])
Sigma_alpha = 0.01 * np.eye(n_alpha)

# State‐space matrices
A = np.zeros((n,n))
A[0,0]      = 1
A[1,1]      = phi1
A[2,2]      = phi2
A[3:,3:]    = Rho

B = np.zeros((n,1))
B[0,0]      = 1
B[1,0]      = 1-phi1
B[2,0]      = 1-phi2

# Imbalance‐penalty constants (λ=1)
# penalty = ½(imb1_next+imb2_next)*x → R_pen and N_pen entries
R_pen = 2 - (phi1 + phi2)
N_pen = np.zeros((n,1))
N_pen[1,0] = 0.5*phi1
N_pen[2,0] = 0.5*phi2

# Full noise covariance (only alphas are noisy)
Sigma_full = np.zeros((n,n))
Sigma_full[3:,3:] = Sigma_alpha

# ---------------------------------------------------
# 2) ALLOCATE BACKWARD VARIABLES
# ---------------------------------------------------
P       = [None] * (T+1)   # P[0],...,P[T]
K       = [None] * T       # K[0],...,K[T-1]
r_const = np.zeros(T+1)    # constants from noise trace

# ---------------------------------------------------
# 3) TERMINAL CONDITION
#    We set V_T(s)=0 so P[T]=0, and then override P[T-1],K[T-1] by hand
# ---------------------------------------------------
P[T] = np.zeros((n,n))
r_const[T] = 0.0

# At t = T-1 = 388: force x = -p  →  K[T-1] = [1,0,...,0]
# And set P[T-1] so that V_{T-1}(s) = ½·(φ1·imb1 + φ2·imb2)·p
P[T-1] = np.zeros((n,n))
P[T-1][0,1] = P[T-1][1,0] = 0.5 * phi1
P[T-1][0,2] = P[T-1][2,0] = 0.5 * phi2
K[T-1]   = np.zeros((1,n))
K[T-1][0,0] = 1.0
r_const[T-1] = 0.5 * np.trace(P[T] @ Sigma_full)  # =0

# ---------------------------------------------------
# 4) BACKWARD RECURSION FOR t = T-2 ... 0
# ---------------------------------------------------
for t in range(T-2, -1, -1):
    # 4.1) build Q_t, N_t, R_t from stds[t]
    Q_t = np.zeros((n,n))
    N_t = N_pen.copy()
    if t < T-39:
        idx, fac = 0, stds[t,0] / 30.0
    elif t < T-9:
        idx, fac = 0, stds[t,0] / (T - 10 - t + 1)
    else:
        idx, fac = 2, stds[t,2] / (T - t + 1)
    # embed the alpha‐P&L term
    Q_t[0,idx] = Q_t[idx,0] = 0.5 * fac
    N_t[0,0]              = fac
    R_t = -R_pen  # note minus because reward‐maximization

    # 4.2) complete the square: H_t = R_t + Bᵀ P[t+1] B
    H = float(R_t + (B.T @ P[t+1] @ B))
    # linear term L = Bᵀ P[t+1] A + N_tᵀ
    L = (B.T @ P[t+1] @ A + N_t.T)
    # compute gains
    K[t] = (L / H)
    # 4.3) Riccati update
    P[t] = (Q_t
            + A.T @ P[t+1] @ A
            - (A.T @ P[t+1] @ B + N_t) @ K[t])
    # noise constant
    r_const[t] = r_const[t+1] + 0.5 * np.trace(P[t+1] @ Sigma_full)

# ---------------------------------------------------
# 5) SIMULATION
# ---------------------------------------------------
np.random.seed(0)
s = np.zeros((n,1))
xs, rewards, values = [], [], []

for t in range(T):
    # 5.1) control
    x = float(-K[t] @ s)
    if t == T-1:
        x = -float(s[0,0])   # force liquidation
    xs.append(x)

    # 5.2) reward
    p = float(s[0,0]); p1 = p + x
    if   t < T-39:
        rate = stds[t,0]/30.0
    elif t < T-9:
        rate = stds[t,0]/(T - 10 - t + 1)
    else:
        rate = stds[t,2]/(T - t + 1)
    im1p = phi1*s[1,0] + (1-phi1)*x
    im2p = phi2*s[2,0] + (1-phi2)*x
    rew   = rate*p1 - 0.5*(im1p + im2p)*x
    rewards.append(rew)

    # 5.3) value
    values.append(0.5 * float(s.T @ P[t] @ s) + r_const[t])

    # 5.4) state update
    eps = np.zeros((n,1))
    eps[3:,0] = np.random.multivariate_normal(np.zeros(n_alpha), Sigma_alpha)
    s = A @ s + B*x + eps

# ---------------------------------------------------
# 6) PLOTS
# ---------------------------------------------------
cum_pos    = np.cumsum(xs)
cum_reward = np.cumsum(rewards)

plt.figure(figsize=(12,4))
plt.subplot(1,3,1)
plt.plot(cum_pos);    plt.title("Cumulative Position"); plt.grid(True)

plt.subplot(1,3,2)
plt.plot(cum_reward); plt.title("Cumulative Reward");   plt.grid(True)

plt.subplot(1,3,3)
plt.plot(values);     plt.title("Value Function V_t(s_t)"); plt.grid(True)

plt.tight_layout()
plt.show()
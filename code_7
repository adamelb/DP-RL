import numpy as np

# =============================================================================
# 1. Sampler function
# =============================================================================
def sampler(n_samples=1000, m_samples=100, mult=1.0, dist="normal"):
    """
    Sample states and parameters.
      - p and alpha: sampled either normally or uniformly (shape: [n_samples, 1, 1])
      - The first sample is forced to zero.
      - corr: sampled from Normal(0.9, 0.02) and clipped.
      - c: uniformly sampled in [0.001, 0.01]
      - t_lambda: uniformly sampled in [0.01, 0.3]
      - next_alpha: for each sample, m_samples are generated using the AR(1) update:
           next_alpha = alpha * corr + sqrt(1 - corr^2)*N(0,1)
    """
    if dist == "normal":
        p = np.random.randn(n_samples, 1, 1) * mult
        alpha = np.random.randn(n_samples, 1, 1) * mult
    elif dist == "uniform":
        p = np.random.uniform(-mult, mult, size=(n_samples, 1, 1))
        alpha = np.random.uniform(-mult, mult, size=(n_samples, 1, 1))
    # Set initial state for first sample to zero:
    p[0, 0, 0] = 0
    alpha[0, 0, 0] = 0

    # Sample correlation from Normal(0.9, 0.02) and clip to (-0.9999, 0.9999)
    corr_arr = np.random.normal(0.9, 0.02, size=(n_samples, 1, 1))
    corr_arr = np.clip(corr_arr, -0.9999, 0.9999)
    
    # Sample cost parameter c uniformly in [0.001, 0.01]
    c_vals = np.random.uniform(0.001, 0.01, size=(n_samples, 1, 1))
    
    # Sample risk penalty t_lambda uniformly in [0.01, 0.3]
    t_lambda_arr = np.random.uniform(0.01, 0.3, size=(n_samples, 1, 1))
    
    # Compute m_samples of next_alpha for each sample:
    next_alpha = alpha * corr_arr + np.sqrt(1 - corr_arr**2) * np.random.randn(n_samples, 1, m_samples)
    
    return p, alpha, next_alpha, c_vals, corr_arr, t_lambda_arr

# =============================================================================
# 2. Feature function for current state (with constant term)
# =============================================================================
def get_features(p, alpha, c_vals, corr_arr, t_lambda_arr):
    """
    Compute features for the current state:
         [1, p^2, alpha^2, c, corr, t_lambda]
    Each input is assumed to be of shape (n_samples, 1, 1); they are squeezed to (n_samples,).
    Returns an array of shape (n_samples, 6).
    """
    n = p.shape[0]
    p_flat     = p[:, 0, 0]
    alpha_flat = alpha[:, 0, 0]
    c_flat     = c_vals[:, 0, 0]
    corr_flat  = corr_arr[:, 0, 0]
    tl_flat    = t_lambda_arr[:, 0, 0]
    constant   = np.ones(n, dtype=np.float64)
    X = np.stack([constant, p_flat**2, alpha_flat**2, c_flat, corr_flat, tl_flat], axis=1)
    return X

# =============================================================================
# 3. Feature function for next state (used in MC evaluation, with constant term)
# =============================================================================
def get_features_next(p_next, next_alpha, c_vals, corr_arr, t_lambda_arr):
    """
    p_next, c_vals, corr_arr, t_lambda_arr are arrays of shape
      (n_samples, num_actions, m_samples)
    next_alpha is also of shape (n_samples, num_actions, m_samples).

    Returns features for the next state as an array of shape
      (n_samples, num_actions, m_samples, 6)
    with features = [ 1, (p_next)^2, (next_alpha)^2, c, corr, t_lambda ].
    """
    feat0 = np.ones_like(p_next)
    feat1 = p_next**2
    feat2 = next_alpha**2
    feat3 = c_vals
    feat4 = corr_arr
    feat5 = t_lambda_arr
    X_next = np.stack([feat0, feat1, feat2, feat3, feat4, feat5], axis=-1)
    return X_next

# =============================================================================
# 4. Reward function
# =============================================================================
def get_reward(alpha, p, actions, c_vals, t_lambda_arr):
    """
    Compute one-step reward for each candidate action.
    alpha, p, c_vals, t_lambda_arr are arrays of shape (n_samples, 1, 1);
    actions is an array of shape (1, num_actions, 1).
    
    Reward: R = alpha*(p+action) - ( c * |action| + 1 ) - 0.5 * t_lambda*(p+action)^2
    Returns an array of shape (n_samples, num_actions, 1).
    """
    return alpha * (p + actions) - (c_vals * np.abs(actions) + 1) - 0.5 * t_lambda_arr * (p + actions)**2

# =============================================================================
# 5. Main ADP loop to fit V using linear regression
# =============================================================================

# Define a grid of candidate actions.
actions = (np.arange(-100, 100).reshape(1, -1, 1)) * 1e-2  # shape: (1, num_actions, 1)

# Settings
n_samples = 1000
m_samples = 100
gamma = 0.95
num_iterations = 200

# Initialize V parameter vector (theta in V = theta^T * features) as zeros; now 6-dimensional.
V = np.zeros(6)  # shape (6,)

for iteration in range(num_iterations):
    # Sample states and parameters.
    p, alpha, next_alpha, c_vals, corr_arr, t_lambda_arr = sampler(n_samples=n_samples, m_samples=m_samples, mult=1.0, dist="normal")
    
    # For candidate actions, compute next state's candidate holdings: p_next = p + action.
    p_next = p + actions  # shape: (n_samples, num_actions, 1)
    
    # Tile next_alpha (original shape: (n_samples,1,m_samples)) along axis 1 (actions dimension)
    next_alpha_tiled = np.tile(next_alpha, (1, actions.shape[1], 1))  # shape: (n_samples, num_actions, m_samples)
    
    # Similarly tile p, c_vals, corr_arr, t_lambda_arr for the next state.
    p_next_tiled = np.tile(p_next, (1, 1, m_samples))  # shape: (n_samples, num_actions, m_samples)
    c_tiled = np.tile(c_vals, (1, actions.shape[1], m_samples))
    corr_tiled = np.tile(corr_arr, (1, actions.shape[1], m_samples))
    tl_tiled = np.tile(t_lambda_arr, (1, actions.shape[1], m_samples))
    
    # Compute features for the next state for each candidate action (MC samples).
    X_next = get_features_next(p_next_tiled, next_alpha_tiled, c_tiled, corr_tiled, tl_tiled)
    # X_next shape: (n_samples, num_actions, m_samples, 6)
    
    # Evaluate V on the next state:
    # For each feature (size 6) take the dot with V.
    V_candidates = np.tensordot(X_next, V, axes=([3],[0]))  # shape: (n_samples, num_actions, m_samples)
    # Average over the m_samples (Monte Carlo) dimension:
    V_next_avg = V_candidates.mean(axis=2, keepdims=True)  # shape: (n_samples, num_actions, 1)
    
    # Compute immediate reward for each candidate action.
    R_immediate = get_reward(alpha, p, actions, c_vals, t_lambda_arr)  # shape: (n_samples, num_actions, 1)
    
    # Candidate Q-value = immediate reward + gamma * estimated next-state value.
    Q_vals = R_immediate + gamma * V_next_avg  # shape: (n_samples, num_actions, 1)
    
    # For each sample, choose the best candidate action (the one with max Q-value).
    best_Q = np.max(Q_vals, axis=1).squeeze()  # shape: (n_samples,)
    
    # (Optionally, extract optimal action indices:
    optimal_index = np.argmax(Q_vals.squeeze(-1), axis=1)  # shape: (n_samples,)
    optimal_action = actions[0, optimal_index, 0]  # shape: (n_samples,)
    
    # Compute features for the CURRENT state.
    X_current = get_features(p, alpha, c_vals, corr_arr, t_lambda_arr)  # shape: (n_samples, 6)
    
    # Update V by linear regression:
    # Solve: X_current * theta â‰ˆ best_Q, using pseudoinverse.
    V = np.linalg.pinv(X_current) @ best_Q  # shape: (6,)
    
    if iteration % 10 == 0:
        print(f"Iteration {iteration}: Mean Q = {best_Q.mean():.4f}, sample optimal actions = {optimal_action[:5]}")

print("Final V parameters:", V)

# =============================================================================
# (Optionally, if you want to include your rollout evaluation code here, you can append it.)
# =============================================================================

# For example, you could run a rollout using a fixed V here.
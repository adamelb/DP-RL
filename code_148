```python
# Paste this into a Jupyter‐Notebook Markdown cell (```python ... ```)

import numpy as np

# 1) PARAMETERS
phi1, phi2 = 0.8, 0.5               # imbalance decay
lmbda = 0.1                        # penalty weight
T = 390                            # number of minutes until close
n_alpha = 3                        # number of alphas

# 2) ALPHA DYNAMICS: AR(1) with triangular ρ
Rho = np.array([[0.9, 0.1, 0.0],
                [0.0, 0.7, 0.2],
                [0.0, 0.0, 0.5]])
Sigma  = np.eye(n_alpha)*0.01      # covariance of ε_t

# 3) BUILD STATE‐SPACE MATRICES
# state s = [ p, imb1, imb2, α₁, α₂, α₃ ]ᵀ
n = 1 + 2 + n_alpha
F = np.zeros((n,n))
F[0,0] = 1                     # pₜ₊₁ = pₜ + xₜ
F[1,1] = phi1                  # imb1ₜ₊₁
F[2,2] = phi2                  # imb2ₜ₊₁
F[3:,3:] = Rho                 # αₜ dynamics

G = np.zeros((n,1))
G[0,0] = 1
G[1,0] = 1 - phi1
G[2,0] = 1 - phi2
# no control enters α‐dynamics

# 4) INSTANTANEOUS COST in quadratic form ℓ(s,x) = ½ sᵀQ s + sᵀN x + ½ R x² + c(s)·x
Q = np.zeros((n,n))            # ℓ contains (p+x)² → Q[0,0]=1
Q[0,0] = 1.0

N = np.zeros((n,1))
# term (imb1ₜ₊₁+imb2ₜ₊₁)*x = [φ₁·imb1 + φ₂·imb2 + (2−φ₁−φ₂)x]·x
# → cross‐terms sᵀN x come from φ·imb
N[1,0] = 0.5*lmbda*phi1
N[2,0] = 0.5*lmbda*phi2

R = 1.0 + lmbda*(2 - (phi1 + phi2))

# 5) TERMINAL COST V_T(s) = −p_T·(imb1_T+imb2_T)  =  sᵀ Qf s
Qf = np.zeros((n,n))
Qf[0,1] = Qf[1,0] = -0.5
Qf[0,2] = Qf[2,0] = -0.5

# 6) BACKWARD RICCATI‐LIKE RECURSION
P = [None]*(T+1)
q = [None]*(T+1)
r = [None]*(T+1)
K = [None]*T

P[T] = Qf
q[T] = np.zeros((n,1))
r[T] = 0.0

for t in reversed(range(T)):
    # 6.1) BUILD M = R + Gᵀ P[t+1] G
    M = float(R + (G.T @ P[t+1] @ G))
    # 6.2) FEEDBACK GAIN K[t]
    K[t] = np.linalg.solve(M, (G.T @ P[t+1] @ F + N.T))
    # 6.3) RICCATI UPDATE for P
    P[t] = (Q 
            + F.T @ P[t+1] @ F 
            - (F.T @ P[t+1] @ G + N) 
              @ np.linalg.solve(M, (G.T @ P[t+1] @ F + N.T))
           )
    # 6.4) (optional) q and r if you need V_t(s) exactly:
    #     c_t = -α₁ₜ/30  (linear term from −α₁/30·(p+x))
    #     q[t] = F.T@(q[t+1] - P[t+1]@G*(c_t/M)) - N*(c_t/M)
    #     r[t] = r[t+1] + 0.5*(c_t**2)/M + 0.5*np.trace(P[t+1]@np.block([[np.zeros((3,3)),None],[None,Sigma]]))

# 7) SIMULATION EXAMPLE
np.random.seed(0)
s = np.zeros((n,1))   # initial state
rewards = []

for t in range(T):
    alpha1 = s[3,0]
    c_t = -alpha1/30
    
    # recompute M and k_t for linear term
    M = float(R + (G.T @ P[t+1] @ G))
    k_t = (c_t / M)
    
    # compute control
    x = float(-K[t] @ s - k_t)
    
    # immediate reward: (α₁/30)*(p+x) - ½(p+x)² - ½λ(imb1ₜ₊₁+imb2ₜ₊₁)x
    p, i1, i2 = s[0,0], s[1,0], s[2,0]
    p1 = p + x
    im1 = phi1*i1 + (1-phi1)*x
    im2 = phi2*i2 + (1-phi2)*x
    rew = alpha1/30*p1 - 0.5*p1**2 - 0.5*lmbda*(im1+im2)*x
    rewards.append(rew)
    
    # state update
    eps = np.zeros((n,1))
    e_alpha = np.random.multivariate_normal(np.zeros(n_alpha), Sigma).reshape(n_alpha,1)
    eps[3:,0] = e_alpha[:,0]
    s = F@s + G*x + eps

# 8) QUICK PLOT
import matplotlib.pyplot as plt
plt.plot(np.cumsum(rewards))
plt.title("Cumulative Reward over One Day")
plt.xlabel("Minute")
plt.ylabel("Cumulative Reward")
plt.grid(True)
plt.show()
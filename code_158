import numpy as np
import matplotlib.pyplot as plt

# 1) PARAMETERS
phi1, phi2 = 0.8, 0.5     # imbalance decay factors
T = 400                   # trading horizon
n_alpha = 3               # number of alpha signals

# 2) AR(1) PARAMETERS FOR ALPHAS
Rho = np.array([[0.9, 0.1, 0.0],
                [0.0, 0.8, 0.1],
                [0.0, 0.0, 0.7]])
Sigma_alpha = 0.01 * np.eye(n_alpha)

# 3) STATE-SPACE MATRICES
# state s = [ p, imb1, imb2, alpha1, alpha2, alpha3 ]ᵀ
n = 1 + 2 + n_alpha
A = np.zeros((n, n))
A[0,0]    = 1
A[1,1]    = phi1
A[2,2]    = phi2
A[3:,3:]  = Rho

B = np.zeros((n,1))
B[0,0] = 1
B[1,0] = 1 - phi1
B[2,0] = 1 - phi2

# 4) IMBALANCE PENALTY (λ=1)
N_pen = np.zeros((n,1))
N_pen[1,0] = 0.5 * phi1
N_pen[2,0] = 0.5 * phi2
R_pen = 2 - (phi1 + phi2)

# 5) ALLOCATE FOR RICCATI RECURSION
P       = [None] * (T+1)
q       = [None] * (T+1)
r_const = np.zeros(T+1)
K       = [None] * T
k       = np.zeros(T)

# Terminal condition: V_T(s) = 0
P[T] = np.zeros((n,n))
q[T] = np.zeros((n,1))
r_const[T] = 0.0

# Precompute noise trace term
Sigma_full = np.zeros((n,n))
Sigma_full[3:,3:] = Sigma_alpha

# 6) BACKWARD RECURSION
for t in range(T-1, -1, -1):
    # Build instantaneous cost matrices (cost = -reward)
    Q_t = np.zeros((n,n))
    N_t = N_pen.copy()
    if   t >= T-9:
        idx, fac = 5, 1.0/(T-t+1)
    elif t >= T-39:
        idx, fac = 3, 1.0/(T-10-t+1)
    else:
        idx, fac = 3, 1.0/100.0
    Q_t[0,idx] = Q_t[idx,0] = -0.5 * fac
    N_t[idx,0] += -fac
    R_t = R_pen

    # Feedback gain
    H = float(R_t + (B.T @ P[t+1] @ B))
    L = (B.T @ P[t+1] @ A + N_t.T)
    d = float(B.T @ q[t+1])
    K[t] = (L / H)
    k[t] = d / H

    # Riccati update
    P[t] = (Q_t
            + A.T @ P[t+1] @ A
            - (A.T @ P[t+1] @ B + N_t) @ K[t])
    q[t] = A.T @ (q[t+1] - P[t+1] @ B * k[t])
    r_const[t] = (r_const[t+1]
                  + 0.5 * (k[t]**2) * H
                  - float(q[t+1].T @ B) * k[t]
                  + 0.5 * np.trace(P[t+1] @ Sigma_full))

# 7) SIMULATION AND PLOTTING
np.random.seed(0)
s = np.zeros((n,1))
xs, rewards, values = [], [], []

for t in range(T):
    # control
    x = float(-K[t] @ s - k[t])
    xs.append(x)

    # reward
    p = float(s[0,0])
    p1 = p + x
    if   t >= T-9:
        rate = s[5,0]/(T-t+1)
    elif t >= T-39:
        rate = s[3,0]/(T-10-t+1)
    else:
        rate = s[3,0]/100.0
    im1p = phi1*s[1,0] + (1-phi1)*x
    im2p = phi2*s[2,0] + (1-phi2)*x
    reward = rate * p1 - 0.5 * (im1p + im2p) * x
    rewards.append(reward)

    # value
    val = 0.5 * float(s.T @ P[t] @ s) + float(q[t].T @ s) + r_const[t]
    values.append(val)

    # state update
    eps = np.zeros((n,1))
    eps[3:,0] = np.random.multivariate_normal(np.zeros(n_alpha), Sigma_alpha)
    s = A @ s + B * x + eps

# cumulative sums
cum_pos = np.cumsum(xs)
cum_rew = np.cumsum(rewards)

plt.figure(figsize=(12,4))
plt.subplot(1,3,1)
plt.plot(cum_pos); plt.title("Cumulative Position"); plt.grid(True)
plt.subplot(1,3,2)
plt.plot(cum_rew); plt.title("Cumulative Reward"); plt.grid(True)
plt.subplot(1,3,3)
plt.plot(values); plt.title("Value V_t(s_t)"); plt.grid(True)
plt.tight_layout()
plt.show()
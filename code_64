"""
GPU-exact dynamic programming for the 2-alpha inventory / market-impact model.
* Tested with: Python 3.10, torch 2.2 (CUDA 12.1), PyTorch 2-.compile on H100-80 GB
* Default grid: 101 points in every continuous dimension  ⇒  43 GB resident VRAM
  (fits comfortably on an 80 GB H100).
* Runs < 30 min for n_vi = 120 and tol = 1e-4 when compiled with torch.compile().
Author: …
"""
from __future__ import annotations
import torch
import math, time

# ───────────────────────── user-tunable parameters ──────────────────────────
device      = torch.device('cuda')           # "cuda:0" by default on single-GPU
dtype       = torch.float32                  # fp32 is still <½ of 80 GB here
grid_pts    = 101                            # 101 → (-3,3) step ≈ 0.06
Nx          = 101                            # number of discrete actions
rho1, rho2  = 0.90, 0.70
C, tla, la  = 5.0, 60.0, 1.0
gamma, phi  = 0.99, 0.90
n_vi        = 120
tol         = 1e-4
torch.manual_seed(0)
# ────────────────────────────────────────────────────────────────────────────

# 1) grids  (all float32 on GPU)
p_space   = torch.linspace(-3,  3, grid_pts,    device=device, dtype=dtype)  # price
a_space   = torch.linspace(-3,  3, grid_pts,    device=device, dtype=dtype)  # α₁, α₂
imb_space = torch.linspace(-.5, .5, grid_pts,   device=device, dtype=dtype)  # inventory
x_space   = torch.linspace(-.5, .5, Nx,         device=device, dtype=dtype)  # actions

Np = Na = Ni = grid_pts

# 2) transition matrices for α₁ & α₂, built with big Monte-Carlo then sent to GPU
def ar1_transition(grid: torch.Tensor, rho: float, n_mc: int = 12_000) -> torch.Tensor:
    """Row-stochastic |grid|×|grid| transition matrix for an AR(1) projected to grid."""
    g = grid.cpu().numpy()                      # MC in NumPy (faster)
    samp = rho*g + math.sqrt(1-rho*rho)*torch.randn(n_mc, g.size).numpy()
    proj = (torch.from_numpy(samp).unsqueeze(-1) - torch.tensor(g)).abs().argmin(-1)
    T = torch.zeros(g.size, g.size, dtype=dtype)
    for i, row in enumerate(proj.T):
        idx, cnt = torch.unique(row, return_counts=True)
        T[i, idx] = cnt.float() / cnt.sum()
    return T.to(device)

T1 = ar1_transition(a_space, rho1)         # (Na, Na)
T2 = ar1_transition(a_space, rho2)

# 3) deterministic next-state index tensors  (int64 on GPU)
def nn_idx(grid: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    """Nearest-neighbour indices along last axis (broadcasting)."""
    return (x.unsqueeze(-1) - grid).abs().argmin(-1)

# broadcast helpers
P  = p_space.view(Np, 1, 1, 1, 1)           # (Np,1,1,1,1)
A1 = a_space.view(1, Na, 1, 1, 1)           # (1,Na,1,1,1)
A2 = a_space.view(1, 1, Na, 1, 1)
I  = imb_space.view(1, 1, 1, Ni, 1)         # (1,1,1,Ni,1)
X  = x_space.view(1, 1, 1, 1, Nx)           # (1,1,1,1,Nx)

# 4) reward tensor  R[s,a]   float32 (≈42 GB at 101⁵)  — fits in 80 GB
pnl    = (A1 + A2) * (P + X)
cost   = 0.5 * tla * X**2 + C * X.abs()
risk   = 0.5 * la  * (P + X)**2
impact = 0.5 * I   * X**2
R      = (pnl - cost - risk - impact).to(dtype)

# 5) next-state indices  (shape (Np,Nx) and (Ni,Nx))
p_next_idx = nn_idx(p_space, P + X).squeeze()[..., :]      # (Np,Nx)
i_next_idx = nn_idx(imb_space, (1-phi)*X + phi*I).squeeze()[..., :]  # (Ni,Nx)

# 6) value function initialised at zero
V  = torch.zeros((Np, Na, Na, Ni), device=device, dtype=dtype)

# ───────────────────── batched GPU value-iteration kernel ───────────────────
@torch.compile(mode="reduce-overhead")      # PyTorch-2.x triton fusion
def value_iter(V: torch.Tensor, n_iter: int, eps: float) -> torch.Tensor:
    T1_, T2_, R_, p_n, i_n = T1, T2, R, p_next_idx, i_next_idx   # capture constants
    for it in range(n_iter):
        V_new = torch.empty_like(V)
        # loop ONLY over actions (Nx=101); everything else is fused-matmul on GPU
        for ix in range(Nx):
            ipn = p_n[:, ix]                     # (Np,)
            iin = i_n[:, ix]                     # (Ni,)
            # advanced indexing → V_next shape (Np,Na,Na,Ni)
            V_next = V[ipn[:, None, None, None], :, :, iin[None, None, None, :]]
            # expected value  E[V′]  :  einsum('aj,bk,pjkf->pabf')
            EV = torch.einsum('aj,bk,pjkf->pabf', T1_, T2_, V_next)
            Q  = R_[..., ix] + gamma * EV
            V_new = Q if ix == 0 else torch.maximum(V_new, Q)
        delta = (V_new - V).abs().max()
        V.copy_(V_new)
        if float(delta) < eps:
            print(f"✓ converged in {it+1} sweeps  (Δ={delta:.2e})")
            break
    return V

# ───────────────────────────── run VI ─────────────────────────────
start = time.time()
V = value_iter(V, n_vi, tol)
print(f"VI run-time: {(time.time()-start)/60:5.1f} minutes")

# ───────────────────── greedy-policy helper (GPU) ──────────────────
@torch.compile(mode="reduce-overhead")
def greedy_action_idx(ip: torch.Tensor, ia1: torch.Tensor,
                      ia2: torch.Tensor, ii: torch.Tensor) -> torch.Tensor:
    """
    Vectorised greedy action for a *batch* of states.
    ip,ia1,ia2,ii are 1-D tensors of identical length B.
    Returns ix ∈ [0,Nx) for each element of the batch (shape (B,))
    """
    B = ip.numel()
    # gather broadcasted slices once per action
    best_val = torch.full((B,), -1e30, device=device, dtype=dtype)
    best_ix  = torch.zeros((B,), device=device, dtype=torch.long)
    for ix in range(Nx):
        ipn = p_next_idx[ip, ix]          # (B,)
        iin = i_next_idx[ii, ix]          # (B,)
        Vn  = V[ipn, :, :, iin]           # (B,Na,Na)
        EV  = torch.einsum('bj,bk,bjk->b', T1[ia1], T2[ia2], Vn)  # (B,)
        q   = R[ip, ia1, ia2, ii, ix] + gamma * EV
        better = q > best_val
        best_val = torch.where(better, q, best_val)
        best_ix  = torch.where(better, torch.tensor(ix, device=device), best_ix)
    return best_ix     # (B,)


# ───────── simulate a 100 000-step trajectory & plot Σ-reward ─────────
import torch, math
import matplotlib.pyplot as plt

T_steps = 100_000
torch.manual_seed(123)

# AR(1) helper — vectorised for speed
def ar1_path(rho: float, n: int, *, device=None, dtype=torch.float32):
    out = torch.empty(n, device=device, dtype=dtype)
    out[0] = torch.randn((), device=device, dtype=dtype)
    sig = math.sqrt(1.0 - rho*rho)
    for t in range(1, n):
        out[t] = rho * out[t-1] + sig * torch.randn((), device=device, dtype=dtype)
    return out.clamp_(-3.0, 3.0)      # keep inside grid

alpha1 = ar1_path(rho1, T_steps, device=device, dtype=dtype)
alpha2 = ar1_path(rho2, T_steps, device=device, dtype=dtype)

# start at p = 0, imbalance = 0   → nearest grid points
ip = (p_space.abs()).argmin().item()
ii = (imb_space.abs()).argmin().item()

rewards = torch.empty(T_steps, device=device, dtype=dtype)

for t in range(T_steps):
    ia1 = (alpha1[t] - a_space).abs().argmin().item()
    ia2 = (alpha2[t] - a_space).abs().argmin().item()

    # greedy optimal action x_t
    ix = greedy_action_idx(
            torch.tensor([ip],  device=device),
            torch.tensor([ia1], device=device),
            torch.tensor([ia2], device=device),
            torch.tensor([ii],  device=device)
         )[0].item()

    # scalar values for the reward components
    x       = x_space[ix].item()
    p_now   = p_space[ip].item()
    imb_now = imb_space[ii].item()

    pnl     = (alpha1[t].item() + alpha2[t].item()) * (p_now + x)
    cost    = 0.5 * tla * x * x + C * abs(x)
    risk    = 0.5 * la  * (p_now + x) ** 2
    impact  = 0.5 * imb_now * x * x
    rewards[t] = pnl - cost - risk - impact

    # move to next discrete state
    ip = p_next_idx[ip, ix].item()
    ii = i_next_idx[ii, ix].item()

# ───────── plot cumulative reward ─────────
cum_rewards = rewards.cumsum(0).cpu()

plt.figure(figsize=(7, 3))
plt.plot(cum_rewards.numpy())
plt.title("Cumulative reward over 100 000 steps under π*")
plt.xlabel("time step")
plt.ylabel("Σ reward")
plt.tight_layout()
plt.show()





# ────────────────── patched value-iteration kernel ──────────────────
@torch.compile(mode="reduce-overhead")
def value_iter(V: torch.Tensor, n_iter: int, eps: float) -> torch.Tensor:
    T1_, T2_, R_, p_n, i_n = T1, T2, R, p_next_idx, i_next_idx   # capture constants
    for it in range(n_iter):
        V_new = torch.empty_like(V)
        for ix in range(Nx):                         # loop over the 101 actions
            ipn = p_n[:, ix]                         # (Np,)
            iin = i_n[:, ix]                         # (Ni,)

            # gather next-state values and **permute to (p, j, k, f)**
            V_next = V[ipn[:, None, None, None], :, :, iin[None, None, None, :]]
            V_next = V_next.permute(0, 2, 3, 1).contiguous()     # (Np, Na, Na, Ni)

            # E[V′]  =  Σ_{j,k}  T1[a,j] · T2[b,k] · V_next[p,j,k,f]
            EV = torch.einsum('aj,bk,pjkf->pabf', T1_, T2_, V_next)

            Q  = R_[..., ix] + gamma * EV
            V_new = Q if ix == 0 else torch.maximum(V_new, Q)

        delta = (V_new - V).abs().max()
        V.copy_(V_new)
        if float(delta) < eps:
            print(f"✓ converged in {it+1} sweeps  (Δ={delta:.2e})")
            break
    return V


        for ix in range(Nx):
            ipn = p_n[:, ix]            # (Np,)
            iin = i_n[:, ix]            # (Ni,)

            # gather next-state values → (Np,1,1,Ni,Na,Na)
            V_next = V[ipn[:, None, None, None], :, :, iin[None, None, None, :]]

            # drop the two broadcasted singleton dimensions → (Np, Ni, Na, Na)
            V_next = V_next.squeeze(1).squeeze(1)

            # reorder to (p, j, k, f) so that 'pjkf' matches the einsum
            V_next = V_next.permute(0, 2, 3, 1).contiguous()

            EV = torch.einsum('aj,bk,pjkf->pabf', T1_, T2_, V_next)
            Q  = R_[..., ix] + gamma * EV
            V_new = Q if ix == 0 else torch.maximum(V_new, Q)



# ► run this immediately after value-iteration, before deleting R ◄
pi_star = torch.empty((Np, Na, Na, Ni), dtype=torch.int16, device=device)

with torch.no_grad():
    best_val = torch.full_like(V, -1e30)          # same shape as V
    for ix in range(Nx):
        ipn = p_next_idx[:, ix]
        iin = i_next_idx[:, ix]
        Vn  = V[ipn[:, None, None, None], :, :, iin[None, None, None, :]] \
                .squeeze(1).squeeze(1)            # (Np,Ni,Na,Na)
        Vn  = Vn.permute(0, 2, 3, 1)              # (p,j,k,f)
        EV  = torch.einsum('aj,bk,pjkf->pabf', T1, T2, Vn)

        Q = R[..., ix] + gamma * EV               # R *used only here*
        better = Q > best_val
        best_val = torch.where(better, Q, best_val)
        pi_star[better] = ix                      # store best x-index

import gc, torch
del R, best_val
gc.collect()
torch.cuda.empty_cache()

def greedy_action_idx(ip: int, ia1: int, ia2: int, ii: int) -> int:
    """Return ix = π*(s) from the pre-computed table (CPU-friendly)."""
    return int(pi_star[ip, ia1, ia2, ii])


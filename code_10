import numpy as np
from typing import Iterable, Sequence, Callable

# Assume NonTerminal is imported from your rl library.

def least_squares_td(
    transitions: Iterable,
    feature_functions: Sequence[Callable[[NonTerminal], float]],
    gamma: float,
    convergence_tolerance: float = 1e-5
):
    """
    Least Squares TD: Given a finite iterable of transitions, update a linear
    function approximator for V: V(s) = theta^T * phi(s), where phi(s)
    is computed from feature_functions.
    
    This version handles transitions that are either tuples (state, next_state, reward)
    or objects with attributes .state, .next_state, .reward.
    """
    num_features: int = len(feature_functions)
    # Regularization for the pseudoinverse (choose Îµ > 0)
    eps = 1e-3
    # Initialize A^{-1} as an identity scaled by 1/eps.
    a_inv: np.ndarray = np.eye(num_features) / eps
    # b vector for the least squares target.
    b_vec: np.ndarray = np.zeros(num_features)
    
    for tr in transitions:
        # Check if tr is a tuple (state, next_state, reward)
        if isinstance(tr, tuple):
            state, next_state, reward = tr  # assume unpacking in that order
        else:
            # Assume tr is an object with attributes state, next_state, reward.
            state, next_state, reward = tr.state, tr.next_state, tr.reward
        
        # Compute features at the current state.
        # We assume state is NonTerminal.
        phi1: np.ndarray = np.array([f(state) for f in feature_functions])
        
        if isinstance(next_state, NonTerminal):
            phi2: np.ndarray = phi1 - gamma * np.array([f(next_state) for f in feature_functions])
        else:
            phi2 = phi1
        
        # Recursive least-squares update:
        temp: np.ndarray = a_inv.T.dot(phi2)
        # Use the Sherman-Morrison update formula:
        a_inv = a_inv - np.outer(a_inv.dot(phi1), temp) / (1 + phi1.dot(temp))
        # Accumulate b_vec
        b_vec += phi1 * reward

    opt_wts: np.ndarray = a_inv.dot(b_vec)
    # Return the weight vector (or wrap it in your LinearFunctionApprox)
    return opt_wts

# Example usage:
if __name__ == "__main__":
    # Suppose we have some dummy transitions.
    # (For example, these might be generated by simulating your MRP.)
    # Here we construct transitions as tuples (state, next_state, reward)
    # where state and next_state are objects of type NonTerminal with a .state attribute.
    
    from rl.markov_process import NonTerminal
    from dataclasses import dataclass
    
    @dataclass(frozen=True)
    class DummyState:
        position: float
        alpha: float
        c: float
        corr: float
        t_lambda: float
    
    # Create a couple of dummy states
    s1 = NonTerminal(DummyState(0, 0.5, 0.005, 0.9,
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D   # noqa: F401

def V1_vec(alpha1, alpha2, p, tl, phi, imbalance):
    A = alpha1 + alpha2
    B = p
    a = 1.5 * tl * (1 - phi)
    b = 1 + tl * phi * imbalance

    if abs(a) < 1e-12:                             # cas quasi-linéaire
        x_star = (A - B) / b
    else:
        c = -(A - B)
        disc = np.maximum(b*b - 4*a*c, 0.0)
        sqrt_disc = np.sqrt(disc)
        x1 = (-b + sqrt_disc) / (2*a)
        x2 = (-b - sqrt_disc) / (2*a)

        def F(x):
            imb_t = (1 - phi)*x + phi*imbalance
            return A*(B + x) - 0.5*tl*imb_t*x**2 - 0.5*(B + x)**2

        x_star = np.where(F(x1) > F(x2), x1, x2)

    imb_t = (1 - phi)*x_star + phi*imbalance
    V = A*(B + x_star) - 0.5*tl*imb_t*x_star**2 - 0.5*(B + x_star)**2
    return V, x_star


# ---- paramètres "figés" ------------------------------------------------------
alpha2    = 0.2
tl        = 0.05
phi       = 0.3
imbalance = 0.0

# grille alpha1 × p
alpha1_vals = np.linspace(-1, 1, 60)
p_vals      = np.linspace(-1, 1, 60)
A_grid, P_grid = np.meshgrid(alpha1_vals, p_vals, indexing='ij')

V_grid, _ = V1_vec(A_grid, alpha2, P_grid, tl, phi, imbalance)

# ---- plot --------------------------------------------------------------------
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(A_grid, P_grid, V_grid, rstride=1, cstride=1,
                linewidth=0, antialiased=False)
ax.set_xlabel(r'$\alpha_1$')
ax.set_ylabel(r'$p$')
ax.set_zlabel(r'$V_1$')
plt.tight_layout(); plt.show()



import math
from typing import Tuple

def V1(alpha1: float,
       alpha2: float,
       p: float,
       tl: float,
       phi: float,
       imbalance: float,
       return_x: bool = False) -> Tuple[float, float]:
    """
    Calcule la value-function d'une itération de Bellman à V(s)=0
    et renvoie le couple (V1, x*).

    Parameters
    ----------
    alpha1, alpha2 : float
        Les deux alphas du pas courant.
    p : float
        Prix courant.
    tl : float
        Coût quadratique de marché (impact).
    phi : float
        Mémoire de l'imbalance (0 <= phi < 1).
    imbalance : float
        Imbalance à t-1.
    return_x : bool
        Si True, renvoie aussi x*.

    Returns
    -------
    V1 : float
        Valeur optimale.
    x_star : float
        Quantité optimale.
    """
    A = alpha1 + alpha2
    B = p
    I = imbalance

    # Coefficients du quadratique a x^2 + b x + c = 0
    a = 1.5 * tl * (1 - phi)
    b = 1 + tl * phi * I
    c = -(A - B)

    # Résolution
    if abs(a) < 1e-12:                       # Equation linéaire
        x_star = (A - B) / b
    else:                                    # Quadratique
        disc = b * b - 4 * a * c
        disc = max(disc, 0.0)               # Sécurité numérique
        x1 = (-b + math.sqrt(disc)) / (2 * a)
        x2 = (-b - math.sqrt(disc)) / (2 * a)
        # Choix de la racine qui maximise F
        def F(x):
            imb_t = (1 - phi) * x + phi * I
            return A * (B + x) - 0.5 * tl * imb_t * x ** 2 - 0.5 * (B + x) ** 2
        x_star = max((x1, x2), key=F)

    # Valeur optimale
    imb_t = (1 - phi) * x_star + phi * I
    V_opt = A * (B + x_star) - 0.5 * tl * imb_t * x_star ** 2 - 0.5 * (B + x_star) ** 2

    return (V_opt, x_star) if return_x else (V_opt, None)


# --- Petit test rapide --------------------------------------
if __name__ == "__main__":
    V, x_opt = V1(0.1, 0.2, p=0.0, tl=0.05, phi=0.3, imbalance=0.0, return_x=True)
    print(f"V1 = {V:.5f},  x* = {x_opt:.5f}")


\[
\begin{aligned}
&X\sim\mathcal{N}(0, I_3),\\
&\text{QR-décomposition :}\quad X = Q\,R,\quad Q^\top Q = I_3,\\
&\lambda_i \overset{\mathrm{iid}}{\sim} \mathcal{U}[0.5,\,1.5],\quad 
 D = \mathrm{diag}(\lambda_1,\lambda_2,\lambda_3),\\
&A = Q\,D\,Q^\top\quad (\text{symétrique définie positive}),\\
&C_{ij} 
 = \frac{A_{ij}}{\sqrt{A_{ii}\,A_{jj}}}
 \quad\Longrightarrow\quad
 C = (C_{ij})_{1\le i,j\le 3}\ \text{est une matrice de corrélation.}
\end{aligned}
\]



Pour simuler un processus $AR(1)$ multivarié avec une matrice de corrélation $\Sigma$, 
on procède ainsi :

\[
\varepsilon_{t+1} \sim \mathcal{N}(0, I),
\quad\quad
\eta_{t+1} = L\,\varepsilon_{t+1}
\quad\quad\text{où}\quad L = \text{Cholesky}(\Sigma)
\]

Ainsi, $\eta_{t+1}$ suit bien une loi $\mathcal{N}(0, \Sigma)$ 
car la décomposition de Cholesky $L$ satisfait $LL^\top = \Sigma$.
On injecte alors ce bruit corrélé dans la dynamique de l'AR(1).




def next_state(p, a, i, x, rho, phi, k, z):        # all tensors on GPU
    """
    p,a,i,x : (B , A_c , 3)
    z      : (q,)
    return     (B , A_c , q , 3)  pour *toutes* les variables
    """
    q = z.numel()

    # ---- bruit corrélé ---------------------------------------------------
    eps = z.view(1,1,q,1)*torch.ones_like(p).unsqueeze(2)   # (B,A_c,q,3)
    eps = eps @ SIG_CH.T

    # ---- ajoute l’axe quantile par .unsqueeze(2) puis .expand ----------
    p_q = p.unsqueeze(2).expand(-1, -1, q, -1)              # (B,A_c,q,3)
    x_q = x.unsqueeze(2).expand_as(p_q)                     # idem

    a_q = a.unsqueeze(2).expand_as(p_q)
    i_q = i.unsqueeze(2).expand_as(p_q)
    rho = rho.unsqueeze(2).expand_as(p_q)
    phi = phi.unsqueeze(2).expand_as(p_q)
    k   = k  .unsqueeze(2).expand_as(p_q)

    # ---- dynamiques ------------------------------------------------------
    a_n = rho * a_q + torch.sqrt(1. - rho**2) * eps
    p_n = p_q + x_q
    i_n = phi * (i_q + k * x_q)

    return p_n, a_n, i_n           # chacun (B , A_c , q , 3)




# ----------------- FIXED next_state ---------------------------------
def next_state(p, a, i, x, rho, phi, k, z):      # all tensors already on GPU
    """
    p,a,i,x : (B , A_c ,      3)
    rho,phi,k : (B , A_c ,    3)
    z      : (q,)  – quantile nodes
    return p',a',i' : (B , A_c , q , 3)
    """
    q = z.numel()

    # ---- add quantile dimension WITHOUT copying data ---------------
    rho = rho.unsqueeze(2)       # (B , A_c , 1 , 3)
    phi = phi.unsqueeze(2)
    k   = k.unsqueeze(2)

    # ---- correlate the standard-normal nodes -----------------------
    eps = z.view(1, 1, q, 1)          # (1 , 1 , q , 1)
    eps = eps.expand(*p.shape[:2], q, 3)          # (B , A_c , q , 3)
    eps = eps @ SIG_CH.T                           # (B , A_c , q , 3)

    # ---- dynamics --------------------------------------------------
    a_n = rho * a.unsqueeze(2) + torch.sqrt(1. - rho**2) * eps
    p_n = p.unsqueeze(2) + x.unsqueeze(2)
    i_n = phi * (i.unsqueeze(2) + k * x.unsqueeze(2))

    return p_n, a_n, i_n            # each (B , A_c , q , 3)


´´´"""
bellman_ddp.py – 8-GPU Bellman power iteration (bfloat16, H100-friendly)

Launch from a notebook *cell* with:
!torchrun --standalone --nproc_per_node 8 bellman_ddp.py
"""

from __future__ import annotations
import math, itertools, os, time, copy, numpy as np
import torch, torch.nn as nn, torch.distributed as dist
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.distributions import Normal

# ──────────────────────────────  CONFIG  ──────────────────────────────
DIM              = 3
A_1D             = 21                       # |A| = 9261
N_QUANT          = 10
BATCH            = 500
N_DATA           = 10_000
N_EPOCH          = 40
N_OUTER          = 6
A_CHUNK_TOTAL    = 512                      # streamed actions per step
Q_CHUNK          = 5
GAMMA            = 0.99
LAMBDA           = 1.0
LR               = 3e-3
DTYPE            = torch.bfloat16           # H100 native
SEED             = 0

torch.manual_seed(SEED); np.random.seed(SEED)

# ──────────────────────────────  helpers  ─────────────────────────────
def truncated_bin_means(N: int, device="cpu", eps=1e-6):
    p = torch.linspace(0., 1., N + 1, device=device)
    lo, up = p[:-1].clamp(eps, 1 - eps), p[1:].clamp(eps, 1 - eps)
    a, b = Normal(0., 1.).icdf(lo), Normal(0., 1.).icdf(up)
    phi = lambda x: torch.exp(-0.5 * x * x) / math.sqrt(2 * math.pi)
    return (phi(a) - phi(b)) / (up - lo)         # (N,)

# –– dataset ––
RHO_MIN, RHO_MAX, C_MIN, C_MAX, T_MIN, T_MAX = 0., 1., 0., 10., 1., 1e3
K_MIN, K_MAX, TAU_MIN, TAU_MAX = 0., 1., 1., 20.

def build_dataset(n=N_DATA):
    p   = torch.empty(n, DIM).uniform_(-1, 1)
    a   = torch.empty_like(p).uniform_(-1, 1)
    i   = torch.zeros_like(p)
    c   = torch.empty_like(p).uniform_(C_MIN, C_MAX)
    t   = torch.empty_like(p).uniform_(T_MIN, T_MAX)
    k   = torch.empty_like(p).uniform_(K_MIN, K_MAX)
    rho = torch.empty_like(p).uniform_(RHO_MIN, RHO_MAX)
    tau = torch.empty_like(p).uniform_(TAU_MIN, TAU_MAX)
    phi = torch.exp(-1. / tau)
    return [x.pin_memory() for x in (p, a, i, c, t, k, rho, phi)]

class CpuTensorDataset(Dataset):
    def __init__(self, tensors): self.tensors = tensors
    def __len__(self): return self.tensors[0].size(0)
    def __getitem__(self, idx): return [t[idx] for t in self.tensors]

# –– features ––
def features(*T):
    tgt = torch.broadcast_shapes(*[x.shape for x in T])
    p, a, i, c, t, k, rho, phi = [x.expand(tgt) for x in T]
    return torch.cat((p, a, i, c, t, k, rho, phi,
                      p.abs(), a.abs(), i.abs(),
                      p.sign(), a.sign()), -1)

F_DIM = features(*[torch.zeros(1, 3)] * 8).size(-1)

# –– sigma ––
def make_sigma(device):
    Q, _ = torch.linalg.qr(torch.randn(DIM, DIM))
    lam  = torch.rand(DIM) + .5
    S    = (Q * lam).matmul(Q.T)
    d    = torch.sqrt(torch.diag(S))
    C    = (S / d.outer(d)).to(device=device, dtype=DTYPE)
    return C, torch.linalg.cholesky(C)

# –– model ––
class ValueMLP(nn.Module):
    def __init__(self, zero: bool = False):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(F_DIM, 128), nn.ReLU(),
            nn.Linear(128, 128),   nn.ReLU(),
            nn.Linear(128, 128),   nn.ReLU(),
            nn.Linear(128, 1)
        )
        if zero:
            for p in self.parameters(): nn.init.zeros_(p)

    def forward(self, x): return self.net(x).squeeze(-1)

# –– reward ––
def reward(alpha, p, impact, x, c, t, k, SIGMA):
    p_new = p + x
    return (alpha * p_new).sum(-1) \
         - (c * x.abs()).sum(-1) \
         - 0.5 * ((t + k) * x ** 2).sum(-1) \
         - 0.5 * LAMBDA * torch.einsum('...i,ij,...j->...', p_new, SIGMA, p_new) \
         - (impact * x).sum(-1)

# –– next-state ––
def next_state(p, a, i, x, rho, phi, k, z, SIG_CH):
    q   = z.numel()
    rho = rho.unsqueeze(2)
    phi = phi.unsqueeze(2)
    k   = k.unsqueeze(2)

    eps = z.view(1, 1, q, 1).expand(*p.shape[:2], q, 3)
    eps = (eps @ SIG_CH.T).to(DTYPE)

    a_n = rho * a.unsqueeze(2) + torch.sqrt(1. - rho ** 2) * eps
    p_n = p.unsqueeze(2) + x.unsqueeze(2)
    i_n = phi * (i.unsqueeze(2) + k * x.unsqueeze(2))
    return p_n, a_n, i_n

# ───────────────────────────  DDP WORKER  ──────────────────────────────
def main_ddp(rank: int, world_size: int):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)
    device = torch.device(f"cuda:{rank}")

    # seed
    torch.manual_seed(SEED + rank)

    # ── constants (build on rank-0, then broadcast) ──
    if rank == 0:
        Z_CPU      = truncated_bin_means(N_QUANT)                       # (Q,)
        ACTION_CPU = torch.tensor(list(itertools.product(
                        torch.linspace(-1., 1., A_1D),
                        torch.linspace(-1., 1., A_1D),
                        torch.linspace(-1., 1., A_1D))), dtype=torch.float32)
    else:
        Z_CPU      = torch.empty(N_QUANT)
        ACTION_CPU = torch.empty(A_1D ** DIM, DIM)

    dist.broadcast(Z_CPU, 0); dist.broadcast(ACTION_CPU, 0)
    A_TOTAL = ACTION_CPU.size(0)

    SIGMA, SIG_CH = make_sigma(device)

    # ── models ──
    target = ValueMLP(zero=True).to(device, dtype=DTYPE).eval()
    model  = ValueMLP().to(device, dtype=DTYPE)
    if torch.cuda.is_available() and torch.__version__ >= "2":
        model = torch.compile(model)          # optional: PyTorch 2+ compile
    model  = nn.parallel.DistributedDataParallel(model, device_ids=[rank])

    # ── data ──
    DATA_CPU = build_dataset()
    dset     = CpuTensorDataset(DATA_CPU)
    sampler  = DistributedSampler(dset, num_replicas=world_size, rank=rank, shuffle=True)
    loader   = DataLoader(dset, batch_size=BATCH, sampler=sampler,
                          pin_memory=True, num_workers=2, persistent_workers=True)

    # ── per-GPU slice of action grid ──
    A_per_gpu = math.ceil(A_TOTAL / world_size)
    a_start   = rank * A_per_gpu
    a_stop    = min(a_start + A_per_gpu, A_TOTAL)
    ACTION_LOCAL_CPU = ACTION_CPU[a_start:a_stop]

    # ── optimiser ──
    opt    = torch.optim.Adam(model.parameters(), lr=LR)
    scaler = torch.cuda.amp.GradScaler()

    # ───────────────────────────  TRAIN  ───────────────────────────────
    for epoch in range(1, N_EPOCH + 1):
        sampler.set_epoch(epoch)

        for batch in loader:
            torch.cuda.nvtx.range_push("batch")
            p, a, i, c, t, k, rho, phi = [t.cuda(device, non_blocking=True, dtype=DTYPE)
                                          for t in batch]

            # candidate value upper-bound
            v_max = torch.full((p.size(0),), -1e30, device=device, dtype=DTYPE)

            # loop over actions (streamed)
            for x_blk_cpu in torch.split(ACTION_LOCAL_CPU, A_CHUNK_TOTAL // world_size):
                x_blk = x_blk_cpu.to(device, dtype=DTYPE)                    # (A_c,3)
                x_blk = x_blk.unsqueeze(0).expand(p.size(0), -1, -1)         # (B,A_c,3)

                expand = lambda z: z.unsqueeze(1).expand_as(x_blk)
                p_b, a_b, i_b, c_b, t_b, k_b, rho_b, phi_b = map(expand, (p, a, i, c, t, k, rho, phi))

                # immediate reward
                r_im = reward(a_b, p_b, i_b, x_blk, c_b, t_b, k_b, SIGMA)

                # loop over quantiles
                v_sum = torch.zeros_like(r_im)
                for z_blk_cpu in torch.split(Z_CPU, Q_CHUNK):
                    z_blk = z_blk_cpu.to(device, dtype=DTYPE)
                    p_n, a_n, i_n = next_state(p_b, a_b, i_b,
                                               x_blk, rho_b, phi_b, k_b,
                                               z_blk, SIG_CH)
                    φ = features(p_n, a_n, i_n,
                                 c_b.unsqueeze(2), t_b.unsqueeze(2),
                                 k_b.unsqueeze(2), rho_b.unsqueeze(2),
                                 phi_b.unsqueeze(2)).to(DTYPE)
                    φ = φ.view(-1, F_DIM)

                    with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):
                        v_blk = target(φ).view(*p_n.shape[:3])
                    v_sum += v_blk.mean(2)

                v_exp = v_sum * (Q_CHUNK / N_QUANT)
                q_val = r_im + GAMMA * v_exp
                v_max = torch.maximum(v_max, q_val.max(1).values)

            # all-reduce MAX
            dist.all_reduce(v_max, op=dist.ReduceOp.MAX)

            # supervise model
            φ0 = features(p, a, i, c, t, k, rho, phi).to(DTYPE)
            with torch.cuda.amp.autocast(dtype=DTYPE):
                v_pred = model(φ0)
                loss   = nn.functional.mse_loss(v_pred, v_max)

            opt.zero_grad(set_to_none=True)
            scaler.scale(loss).backward()
            scaler.step(opt); scaler.update()
            torch.cuda.nvtx.range_pop()

        if rank == 0:
            print(f"[outer ?  epoch {epoch:02d}] loss={loss.item():.5f}")

    # ── save weights once ──
    state = model.module.state_dict()
    torch.save(state, f"V_tmp_rank{rank}.pt")
    dist.barrier()                                 # wait for all ranks
    dist.destroy_process_group()

# ──────────────────────────────  ENTRY  ────────────────────────────────
if __name__ == "__main__":
    # Spawn only when launched *as a script* (i.e. under torchrun)
    world_size = torch.cuda.device_count()
    torch.multiprocessing.spawn(main_ddp, args=(world_size,),
                                nprocs=world_size, join=True)
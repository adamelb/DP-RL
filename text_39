## Decomposing a unit‐variance AR(0.7) process into four AR(ρᵢ) components

Below we start from the AR(1) process
\[
X_{t+1} \;=\; 0.7\,X_t \;+\;\sqrt{\,1 - 0.7^2\,}\;\eta_{t+1},
\qquad \eta_{t+1}\;\overset{\text{i.i.d.}}{\sim}\;\mathcal{N}(0,1),
\]
which is by construction stationary with 
\[
\mathrm{Var}(X_t)=1,\quad
\mathrm{Cov}(X_t,X_{t-h}) \;=\; 0.7^{\,|h|},\;\;h=0,1,2,\dots\,.
\]
We would like to write
\[
X_t \;=\;\sum_{i=1}^{4} Y_t^{(i)},
\]
where each \(\{Y^{(i)}\}\) is itself an AR(1) process with parameter \(\rho_i\) (but \emph{not} necessarily unit‐variance).  Concretely, for \(i=1,2,3,4\) set
\[
Y_{t+1}^{(i)} \;=\;\rho_i\,Y_{t}^{(i)} \;+\;\varepsilon_{t+1}^{(i)},
\qquad 
\varepsilon_{t+1}^{(i)}\;\overset{\text{i.i.d.}}{\sim}\;\mathcal{N}\bigl(0,\sigma_i^2\bigr),
\]
so that its stationary variance is
\[
\mathrm{Var}\bigl(Y_t^{(i)}\bigr)
\;=\;\frac{\sigma_i^2}{\,1 - \rho_i^2\,}, 
\quad\text{and}\quad
\mathrm{Cov}\bigl(Y_t^{(i)},\,Y_{t-h}^{(i)}\bigr)
=\frac{\sigma_i^2}{\,1 - \rho_i^2\,}\;\rho_i^{\,|h|}.
\]
We fix
\[
\rho_1=0,\quad \rho_2=0.44,\quad \rho_3=0.85,\quad \rho_4=0.95,
\]
and choose \(\sigma_i^2\ge0\) (hence \(\mathrm{Var}(Y^{(i)})\ge0\)) and require
\[
X_t \;=\;\sum_{i=1}^4 Y_t^{(i)} 
\quad\Longrightarrow\quad
\mathrm{Cov}(X_t,X_{t-h})
=\sum_{i=1}^4 \mathrm{Cov}\bigl(Y_t^{(i)},\,Y_{t-h}^{(i)}\bigr),
\]
for all \(h\).  Since \(X_t\) has \(\mathrm{Cov}(X_t,X_{t-h})=0.7^{\,|h|}\), we must have
\[
\sum_{i=1}^4 \frac{\sigma_i^2}{\,1 - \rho_i^2\,}\;\rho_i^{\,|h|}
\;=\;0.7^{\,|h|}, 
\quad h=0,1,2,\dots
\]
Define 
\[
w_i \;:=\; \frac{\sigma_i^2}{\,1 - \rho_i^2\,}\;\ge0,\qquad i=1,2,3,4.
\]
Then for any \(h\ge0\),
\[
\mathrm{Cov}(X_t,X_{t-h})
=\sum_{i=1}^4 w_i\,\rho_i^{\,|h|}.
\]
In particular:

1.  **Lag 0 (variance):**  
    \[
    \mathrm{Var}(X_t) 
    = \sum_{i=1}^4 w_i\,\rho_i^0 
    = \sum_{i=1}^4 w_i 
    \stackrel{!}{=} 1.
    \]
2.  **Lag 1:**
    \[
    \mathrm{Cov}(X_t,X_{t-1})
    = \sum_{i=1}^4 w_i\,\rho_i 
    \stackrel{!}{=} 0.7.
    \]
3.  **Lag 2:**
    \[
    \mathrm{Cov}(X_t,X_{t-2})
    = \sum_{i=1}^4 w_i\,\rho_i^2 
    \stackrel{!}{=} 0.7^2 = 0.49.
    \]
4.  **Lag 3:**
    \[
    \mathrm{Cov}(X_t,X_{t-3})
    = \sum_{i=1}^4 w_i\,\rho_i^3 
    \stackrel{!}{=} 0.7^3 = 0.343.
    \]
Hence the four nonnegative weights \(\{w_i\}\) must satisfy
\[
\begin{cases}
w_1 + w_2 + w_3 + w_4 \;=\; 1,\\[0.5em]
w_1\,\rho_1 + w_2\,\rho_2 + w_3\,\rho_3 + w_4\,\rho_4 \;=\; 0.7,\\[0.5em]
w_1\,\rho_1^2 + w_2\,\rho_2^2 + w_3\,\rho_3^2 + w_4\,\rho_4^2 \;=\; 0.49,\\[0.5em]
w_1\,\rho_1^3 + w_2\,\rho_2^3 + w_3\,\rho_3^3 + w_4\,\rho_4^3 \;=\; 0.343,
\end{cases}
\]
with \((\rho_1,\rho_2,\rho_3,\rho_4)=(0,\,0.44,\,0.85,\,0.95)\).  Because \(\rho_1=0\), any \(\rho_1^h=0\) for \(h\ge1\).  Concretely:

- From the first line,
  \[
  w_1 \;=\; 1 \;-\;(w_2 + w_3 + w_4).
  \]
- Then the equations for \(h=1,2,3\) involve only \(w_2,w_3,w_4\):
  \[
  \begin{cases}
  0.44\,w_2 \;+\; 0.85\,w_3 \;+\; 0.95\,w_4 = 0.7,\\[0.5em]
  0.44^2\,w_2 \;+\; 0.85^2\,w_3 \;+\; 0.95^2\,w_4 = 0.49,\\[0.5em]
  0.44^3\,w_2 \;+\; 0.85^3\,w_3 \;+\; 0.95^3\,w_4 = 0.343.
  \end{cases}
  \]
  Numerically:
  \[
  \begin{pmatrix}
  0.44 & 0.85 & 0.95\\[0.3em]
  0.1936 & 0.7225 & 0.9025\\[0.3em]
  0.085184 & 0.614125 & 0.857375
  \end{pmatrix}
  \begin{pmatrix}w_2\\w_3\\w_4\end{pmatrix}
  \;=\;
  \begin{pmatrix}0.7\\0.49\\0.343\end{pmatrix}.
  \]
  Inverting this \(3\times3\) matrix gives the unique solution
  \[
  \bigl(w_2,w_3,w_4\bigr)
  \;\approx\;\bigl(0.2853137,\;1.3055954,\;-0.5634675\bigr).
  \]
- Then 
  \[
  w_1 \;=\; 1 \;-\;(w_2 + w_3 + w_4)
  \;\approx\; 1 \;-\;(0.2853137 + 1.3055954 - 0.5634675)
  \;=\;-0.0274416.
  \]
Hence the unique algebraic solution is
\[
\bigl(w_1,w_2,w_3,w_4\bigr)
\;\approx\;
\bigl(-0.0274416,\;0.2853137,\;1.3055954,\;-0.5634675\bigr).
\]
Because \(w_1<0\) and \(w_4<0\), there is **no** choice of nonnegative \(\{\sigma_i^2\}\) that yields \(w_i=\sigma_i^2/(1-\rho_i^2)\).  Equivalently:

> **No decomposition into four real AR\((\rho_i)\) processes** (with \(\rho_i\in\{0,0.44,0.85,0.95\}\)) can reproduce the exact autocovariance \(\mathrm{Cov}(X_t,X_{t-h})=0.7^{\,|h|}\) for all \(h=0,1,2,3\).  

---

### Consequence and approximate alternatives

1.  **Exact decomposition is impossible.**  
    Because the four fixed values \(\{\rho_i\}\) do not include \(0.7\) itself, any attempt to write
    \[
    0.7^h \;=\;\sum_{i=1}^4 w_i\,\rho_i^h
    \quad(\text{for all }h\ge0)
    \]
    leads to a linear system in \(\{w_i\}\) with no real nonnegative solution.  In particular, matching lags \(0,1,2,3\) already forces some \(w_i<0\).  

2.  **Matching fewer lags.**  
    - If we only insist on matching \(\mathrm{Var}(X_t)=1\) and \(\mathrm{Cov}(X_t,X_{t-1})=0.7\), then we have two equations:
      \[
      \begin{cases}
      w_1 + w_2 + w_3 + w_4 = 1,\\[0.5em]
      0.44\,w_2 + 0.85\,w_3 + 0.95\,w_4 = 0.7,
      \end{cases}
      \]
      with \(w_i\ge0\).  That system has infinitely many nonnegative solutions.  For instance, one convenient choice is
      \[
      w_1=0,\quad w_2=0,\quad
      w_3=0.8235294,\quad w_4=0.1764706,
      \]
      since
      \[
      0.8235294 + 0.1764706 = 1,\qquad
      0.85\cdot 0.8235294 + 0.95\cdot 0.1764706 = 0.7.
      \]
      Then set
      \[
      \sigma_3^2 = (1 - 0.85^2)\,w_3,\quad
      \sigma_4^2 = (1 - 0.95^2)\,w_4,
      \]
      and
      \(\sigma_1^2=\sigma_2^2=0\).  Concretely,
      \[
      \mathrm{Var}(Y^{(3)}) 
      = \frac{\sigma_3^2}{\,1 - 0.85^2\,} = w_3=0.8235294,
      \quad
      \mathrm{Var}(Y^{(4)}) = w_4=0.1764706.
      \]
      This choice yields
      \[
      X_t = Y_t^{(3)} + Y_t^{(4)}, 
      \]
      which is an AR‐sum whose lag‐1 autocovariance is exactly \(0.7\) and whose variance is exactly 1—but it will not match \(\mathrm{Cov}(X_t,X_{t-2})=0.49\) or higher lags.  

3.  **Interpretation.**  
    - If your only goal is to replicate “an AR(0.7)‐like” process up to lag 1, you may choose any nonnegative \(\{w_i\}\) satisfying the two equations above.  
    - If you want to match lags \(0,1,2\) (so that \(\rho_X(2)=0.49\) as well), then you have three linear equations in \(\{w_1,\dots,w_4\}\).  That system still has infinitely many solutions (one degree of freedom), and one can pick a nonnegative solution easily.  However, as soon as you force a fourth equation (\(\rho_X(3)=0.343\)), there is no nonnegative solution.  

---

### Summary

- **Exact four‐component decomposition** (for all lags \(h\)) of an AR(0.7) process into four AR\(\bigl(\rho_i\bigr)\) signals with \(\rho_i\in\{0,0.44,0.85,0.95\}\) does not exist (because the resulting weight‐system forces some “weights” negative).  
- **Partial (approximate) decompositions** are possible if you match only the first one or two lags.  For instance, matching lags \(0\) and \(1\) exactly gives infinitely many nonnegative weights; one explicit choice is
  \[
  w_3=0.8235294,\;w_4=0.1764706,\;w_1=w_2=0,
  \]
  so that
  \[
  X_t \;=\; Y_t^{(3)} \;+\; Y_t^{(4)},
  \]
  where
  \[
  Y_{t+1}^{(3)} \;=\; 0.85\,Y_t^{(3)} + \varepsilon_{t+1}^{(3)},\quad
  \mathrm{Var}\bigl(\varepsilon_{t+1}^{(3)}\bigr)
  = (1 - 0.85^2)\cdot 0.8235294,
  \]
  \[
  Y_{t+1}^{(4)} \;=\; 0.95\,Y_t^{(4)} + \varepsilon_{t+1}^{(4)},\quad
  \mathrm{Var}\bigl(\varepsilon_{t+1}^{(4)}\bigr)
  = (1 - 0.95^2)\cdot 0.1764706,
  \]
  and \(Y^{(1)}\equiv Y^{(2)}\equiv0\).  This ensures 
  \(\mathrm{Var}(X_t)=1\) and \(\mathrm{Cov}(X_t,X_{t-1})=0.7\), but \(\mathrm{Cov}(X_t,X_{t-2})\neq0.49\).  

Hence, **to “build” four AR processes of given \(\rho_i\) that sum to your AR(0.7) signal, you must choose how many lags you want to match exactly.  Matching all lags up to 3 forces negative variance‐weights, so no real solution exists.  Matching only lag‐1 (or lag‐2) gives a valid nonnegative solution, and you then set \(\sigma_i^2 = (1-\rho_i^2)\,w_i\) and let**
\[
Y_{t+1}^{(i)} = \rho_i\,Y_t^{(i)} + \varepsilon_{t+1}^{(i)},
\quad \mathrm{Var}\bigl(\varepsilon_{t+1}^{(i)}\bigr) = \sigma_i^2,
\quad
X_t = \sum_{i=1}^4 Y_t^{(i)}.
\]
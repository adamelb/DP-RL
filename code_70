
from scipy.linalg import solve_discrete_are

# Define Q and R matrices explicitly for scipy's solve_discrete_are
# We extract these from the immediate reward function R(z, x) = -0.5 * [z; x]^T H [z; x]

# Block decomposition of H:
Q = H[:-1, :-1]
N = H[:-1, -1:]
R = H[-1:, -1:]

# Convert into scipy format: solve P from DARE
# SciPy solves: A^T P A - A^T P B (R + B^T P B)^-1 B^T P A + Q = P

# Use solve_discrete_are
P_scipy = solve_discrete_are(A, B, Q, R, s=N, e=None)

# Compute feedback gain K
R_eff = R + B.T @ P_scipy @ B
K_scipy = np.linalg.inv(R_eff) @ (N.T + B.T @ P_scipy @ A)

# Compute the value function constant term
c_scipy = (gamma / (1 - gamma)) * np.trace(P_scipy @ Sigma_w)

P_scipy, K_scipy, c_scipy



import numpy as np

# Define constants
phi = 0.8
tilde_l = 0.5
gamma = 0.95
rho1 = 0.9
rho2 = 0.85

# Define matrices
A = np.array([
    [1, 0,   0,   0],
    [0, phi, 0,   0],
    [0, 0,   rho1, 0],
    [0, 0,   0,   rho2]
])

B = np.array([
    [1],
    [1 - phi],
    [0],
    [0]
])

# Shock covariance
Sigma_w = np.diag([0, 0, 1 - rho1**2, 1 - rho2**2])

# Define the H matrix
H = np.array([
    [1, 0, 0, 0, -1],
    [0, 0, 0, 0, (tilde_l/2)*(1 - phi)],
    [0, 0, 0, 0, -1],
    [0, 0, 0, 0, -1],
    [-1, (tilde_l/2)*(1 - phi), -1, -1, 1 + (tilde_l/2)*(1 - phi)**2]
])

# Function to compute the Riccati update
def riccati_update(P):
    AB = np.hstack((A @ P @ A.T, A @ P @ B))
    BA = np.hstack((B.T @ P @ A, B.T @ P @ B))
    top = np.hstack((A.T @ P @ A, A.T @ P @ B))
    bottom = np.hstack((B.T @ P @ A, B.T @ P @ B))
    APB = np.block([[A.T @ P @ A, A.T @ P @ B],
                    [B.T @ P @ A, B.T @ P @ B]])
    
    Xi = H - gamma * APB
    
    Q = Xi[:-1, :-1]
    N = Xi[:-1, -1:]
    R = Xi[-1:, -1:]

    R_inv = np.linalg.inv(R + gamma * B.T @ P @ B)
    K = R_inv @ (N.T + gamma * B.T @ P @ A)
    
    P_next = Q + gamma * A.T @ P @ A - gamma * A.T @ P @ B @ R_inv @ B.T @ P @ A
    return P_next, K

# Iterate Riccati equation
P = np.zeros((4, 4))
tolerance = 1e-8
max_iter = 500
for i in range(max_iter):
    P_next, K = riccati_update(P)
    if np.linalg.norm(P_next - P) < tolerance:
        break
    P = P_next

# Compute value function constant term
c = (gamma / (1 - gamma)) * np.trace(P @ Sigma_w)

P, K, c

# Add clipping to prevent overflow in p, imb, x, etc.
# Also limit extreme exponential growth in discounting

z = np.array([0.0, 0.0, 0.0, 0.0])
cumulative_reward = 0.0
log_discount = 0.0
max_abs_val = 1e3  # limit state and action magnitudes to prevent explosion

for t in range(T):
    x = float(np.clip(-K @ z, -max_abs_val, max_abs_val))

    # Clip state variables to avoid blow-up
    alpha_sum = np.clip(z[2] + z[3], -max_abs_val, max_abs_val)
    p_next = np.clip(z[0] + x, -max_abs_val, max_abs_val)
    imb_next = np.clip(phi * z[1] + (1 - phi) * x, -max_abs_val, max_abs_val)
    reward = alpha_sum * p_next - 0.5 * tilde_l * (phi * z[1] + (1 - phi) * x) * x - 0.5 * (p_next)**2

    # Safely update cumulative reward
    discount = np.exp(log_discount) if log_discount > -700 else 0.0
    cumulative_reward += discount * reward

    log_discount += np.log(gamma)

    epsilon1 = np.random.randn()
    epsilon2 = np.random.randn()
    alpha1_next = float(np.clip(rho1 * z[2] + np.sqrt(1 - rho1**2) * epsilon1, -max_abs_val, max_abs_val))
    alpha2_next = float(np.clip(rho2 * z[3] + np.sqrt(1 - rho2**2) * epsilon2, -max_abs_val, max_abs_val))

    z = np.array([p_next, imb_next, alpha1_next, alpha2_next])

cumulative_reward

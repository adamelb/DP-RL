
# Fix symbolic term generation â€” we must compare symbols by name, not relational operators

# Generate quadratic basis terms manually without comparisons
linear_terms = [p, imb, a1, a2]
quadratic_terms = [
    p**2, imb**2, a1**2, a2**2,
    p*imb, p*a1, p*a2, imb*a1, imb*a2, a1*a2
]
terms = [1] + linear_terms + quadratic_terms  # total of 15 terms

# Create lambdified version of V_opt
V_func = sp.lambdify((p, imb, a1, a2, *c), V_opt, 'numpy')

# Build linear system A x = b
A = []
b = []
samples = 50
grid = np.linspace(-1, 1, int(np.ceil(samples ** 0.25)))

for p0 in grid:
    for imb0 in grid:
        for a10 in grid:
            for a20 in grid:
                row = [sp.lambdify((p, imb, a1, a2), term, 'numpy')(p0, imb0, a10, a20) for term in terms]
                val = V_func(p0, imb0, a10, a20, *([0]*15))
                A.append(row)
                b.append(val)
                if len(A) >= 100:
                    break
            if len(A) >= 100: break
        if len(A) >= 100: break
    if len(A) >= 100: break

A = np.array(A)
b = np.array(b)

# Solve least squares system
coeffs, *_ = np.linalg.lstsq(A, b, rcond=None)
coeff_dict = dict(zip([str(term) for term in terms], coeffs))

coeff_dict




from scipy.linalg import solve_discrete_are

# Define Q and R matrices explicitly for scipy's solve_discrete_are
# We extract these from the immediate reward function R(z, x) = -0.5 * [z; x]^T H [z; x]

# Block decomposition of H:
Q = H[:-1, :-1]
N = H[:-1, -1:]
R = H[-1:, -1:]

# Convert into scipy format: solve P from DARE
# SciPy solves: A^T P A - A^T P B (R + B^T P B)^-1 B^T P A + Q = P

# Use solve_discrete_are
P_scipy = solve_discrete_are(A, B, Q, R, s=N, e=None)

# Compute feedback gain K
R_eff = R + B.T @ P_scipy @ B
K_scipy = np.linalg.inv(R_eff) @ (N.T + B.T @ P_scipy @ A)

# Compute the value function constant term
c_scipy = (gamma / (1 - gamma)) * np.trace(P_scipy @ Sigma_w)

P_scipy, K_scipy, c_scipy



import numpy as np

# Define constants
phi = 0.8
tilde_l = 0.5
gamma = 0.95
rho1 = 0.9
rho2 = 0.85

# Define matrices
A = np.array([
    [1, 0,   0,   0],
    [0, phi, 0,   0],
    [0, 0,   rho1, 0],
    [0, 0,   0,   rho2]
])

B = np.array([
    [1],
    [1 - phi],
    [0],
    [0]
])

# Shock covariance
Sigma_w = np.diag([0, 0, 1 - rho1**2, 1 - rho2**2])

# Define the H matrix
H = np.array([
    [1, 0, 0, 0, -1],
    [0, 0, 0, 0, (tilde_l/2)*(1 - phi)],
    [0, 0, 0, 0, -1],
    [0, 0, 0, 0, -1],
    [-1, (tilde_l/2)*(1 - phi), -1, -1, 1 + (tilde_l/2)*(1 - phi)**2]
])

# Function to compute the Riccati update
def riccati_update(P):
    AB = np.hstack((A @ P @ A.T, A @ P @ B))
    BA = np.hstack((B.T @ P @ A, B.T @ P @ B))
    top = np.hstack((A.T @ P @ A, A.T @ P @ B))
    bottom = np.hstack((B.T @ P @ A, B.T @ P @ B))
    APB = np.block([[A.T @ P @ A, A.T @ P @ B],
                    [B.T @ P @ A, B.T @ P @ B]])
    
    Xi = H - gamma * APB
    
    Q = Xi[:-1, :-1]
    N = Xi[:-1, -1:]
    R = Xi[-1:, -1:]

    R_inv = np.linalg.inv(R + gamma * B.T @ P @ B)
    K = R_inv @ (N.T + gamma * B.T @ P @ A)
    
    P_next = Q + gamma * A.T @ P @ A - gamma * A.T @ P @ B @ R_inv @ B.T @ P @ A
    return P_next, K

# Iterate Riccati equation
P = np.zeros((4, 4))
tolerance = 1e-8
max_iter = 500
for i in range(max_iter):
    P_next, K = riccati_update(P)
    if np.linalg.norm(P_next - P) < tolerance:
        break
    P = P_next

# Compute value function constant term
c = (gamma / (1 - gamma)) * np.trace(P @ Sigma_w)

P, K, c

# Add clipping to prevent overflow in p, imb, x, etc.
# Also limit extreme exponential growth in discounting

z = np.array([0.0, 0.0, 0.0, 0.0])
cumulative_reward = 0.0
log_discount = 0.0
max_abs_val = 1e3  # limit state and action magnitudes to prevent explosion

for t in range(T):
    x = float(np.clip(-K @ z, -max_abs_val, max_abs_val))

    # Clip state variables to avoid blow-up
    alpha_sum = np.clip(z[2] + z[3], -max_abs_val, max_abs_val)
    p_next = np.clip(z[0] + x, -max_abs_val, max_abs_val)
    imb_next = np.clip(phi * z[1] + (1 - phi) * x, -max_abs_val, max_abs_val)
    reward = alpha_sum * p_next - 0.5 * tilde_l * (phi * z[1] + (1 - phi) * x) * x - 0.5 * (p_next)**2

    # Safely update cumulative reward
    discount = np.exp(log_discount) if log_discount > -700 else 0.0
    cumulative_reward += discount * reward

    log_discount += np.log(gamma)

    epsilon1 = np.random.randn()
    epsilon2 = np.random.randn()
    alpha1_next = float(np.clip(rho1 * z[2] + np.sqrt(1 - rho1**2) * epsilon1, -max_abs_val, max_abs_val))
    alpha2_next = float(np.clip(rho2 * z[3] + np.sqrt(1 - rho2**2) * epsilon2, -max_abs_val, max_abs_val))

    z = np.array([p_next, imb_next, alpha1_next, alpha2_next])

cumulative_reward






import numpy as np
from scipy.linalg import solve_discrete_are

# === Step 1: Define constants ===
phi = 0.8
tilde_l = 0.5
gamma = 0.95
rho1 = 0.9
rho2 = 0.85

# === Step 2: Define system matrices ===
A = np.array([
    [1, 0,   0,   0],
    [0, phi, 0,   0],
    [0, 0,   rho1, 0],
    [0, 0,   0,   rho2]
])

B = np.array([
    [1],
    [1 - phi],
    [0],
    [0]
])

Sigma_w = np.diag([0, 0, 1 - rho1**2, 1 - rho2**2])

# === Step 3: Define stage cost matrix H ===
H = np.array([
    [1, 0, 0, 0, -1],
    [0, 0, 0, 0, (tilde_l/2)*(1 - phi)],
    [0, 0, 0, 0, -1],
    [0, 0, 0, 0, -1],
    [-1, (tilde_l/2)*(1 - phi), -1, -1, 1 + (tilde_l/2)*(1 - phi)**2]
])

Q = H[:-1, :-1]
N = H[:-1, -1:]
R = H[-1:, -1:]

# === Step 4: Solve DARE using SciPy ===
P = solve_discrete_are(A, B, Q, R, s=N)
R_eff = R + B.T @ P @ B
K = np.linalg.inv(R_eff) @ (N.T + B.T @ P @ A)

# === Step 5: Value function constant term ===
c = (gamma / (1 - gamma)) * np.trace(P @ Sigma_w)

# === Step 6: Simulate trajectory and compute cumulative reward ===
T = 100_000
z = np.array([0.0, 0.0, 0.0, 0.0])  # initial state
log_discount = 0.0
cumulative_reward = 0.0
max_abs_val = 1e3

for t in range(T):
    x = float(np.clip(-K @ z, -max_abs_val, max_abs_val))

    alpha_sum = np.clip(z[2] + z[3], -max_abs_val, max_abs_val)
    p_next = np.clip(z[0] + x, -max_abs_val, max_abs_val)
    imb_next = np.clip(phi * z[1] + (1 - phi) * x, -max_abs_val, max_abs_val)
    reward = alpha_sum * p_next - 0.5 * tilde_l * (phi * z[1] + (1 - phi) * x) * x - 0.5 * (p_next)**2

    discount = np.exp(log_discount) if log_discount > -700 else 0.0
    cumulative_reward += discount * reward
    log_discount += np.log(gamma)

    epsilon1 = np.random.randn()
    epsilon2 = np.random.randn()
    alpha1_next = float(np.clip(rho1 * z[2] + np.sqrt(1 - rho1**2) * epsilon1, -max_abs_val, max_abs_val))
    alpha2_next = float(np.clip(rho2 * z[3] + np.sqrt(1 - rho2**2) * epsilon2, -max_abs_val, max_abs_val))

    z = np.array([p_next, imb_next, alpha1_next, alpha2_next])

# === Final result ===
print("Optimal feedback gain K:", K)
print("Quadratic value matrix P:\n", P)
print("Value function constant term c =", c)
print("Discounted cumulative reward over 100,000 steps =", cumulative_reward)


